---
layout: post
title: "ICML 2017论文马拉松"
tags: [论文笔记]
excerpt: "给浏览的ICML 2017的论文做一个笔记，由于文章太多了，只选择了我自己感兴趣的领域去读。"
date: 2017-05-27 16:19:00
mathjax: true
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

吸引力指数：指数类型为整型，取值范围[0-5]，值越大表明在浏览完文章后想要精读的冲动程度大小。

#### 模型

1.《Fast and Provably Good Seedings for k-Means》

(_这篇文章是NIPS 2016上的文章，同一拨人还做了一篇AAAI的文章。_)

关于K-means算法的研究有两个主要问题：第一个是初始中心点的选择，第二个是聚类个数的选择。这篇文章作者提出了**Assumption-free K-MC^2**算法解决第一个问题。k-menas++算法已经是state-of-art的算法了，但是该算法内在的要求遍历K轮整个数据集，因而不适合大数据集。有人提出使用Markov Chain Monte Carlo采样来近似k-means++中seeding的选择，但是该方法要求对数据分布有要求。作者就是在该算法基础上提出了对数据分布没有假设依赖的算法，同时给出了理论和实验的保证。作者已经在Gihub上放出了代码(Cython)。

2.《How close are the eigenvectors and eigenvalues of the sample and actual covariance matrices?》

这篇文章主要提供了一种潜在的降维技术，作者通过分析部分样本的特征值和特征向量与全部样本的特征值和特征向量之间的gap，讨论要达到和全部样本几乎相同的特征值和特征向量需要的样本数量。文章有大量的数学推理和证明，不过基本概念并不复杂，对降维技术感兴趣可以读。

3.《Communication-eﬃcient Algorithms for Distributed Stochastic Principal Component Analysis》

PCA降维技术的改进分布式方案

4.《Canopy – Fast Sampling using Cover Trees》

一种基于cover tree的快速采样算法，可以用于大数据背景下的搜索和聚类环境。记得之前有基于cover tree的最近邻搜索算法。

5.《Learning Gradient Descent: Better Generalization and Longer Horizons》

Learning to Learn的又一作品。可以和DeepMind的那篇文章结合看，同样今年DeepMind又出了一篇类似的文章。作者提出了一种新的模型**RNNProp**，此外有一个重要的trick是Random Scaling,据文章说可以有效提高模型的泛化性能。


#### 优化

1.《SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient》

提出了一种新的梯度优化算法SAPAH,改进SAGA和SVRG。该算法的step方向并不是沿着随机梯度方向，而是由历史随机梯度信息（类似SAGA）和随机完全梯度信息(类似SVRG)合成的方向。文章证明了对光滑且凸的目e标函数，为次线性收敛；对强凸，线性收敛且计算复杂度和SVRG与SAG一致。同时证明比起SVRG，SAPAH有更小的收敛常数和更好的稳定性。

2.《How to Escape Saddle Points Efficiently?》

吸引力指数: 3 

这篇文章提出一个"Dimension Free"的分析，文章证明，**带扰动的梯度下降**收敛到**二阶驻点**需要的迭代次数满足对维度的多项式对数依赖，同时该过程的收敛率和**梯度下降收敛到一阶驻点**相同。结论的得出依赖于对saddle point附近的几何特征的分析。文章的成果应用于矩阵分解任务中可以获得一个更快的全局收敛率。

3.《An Efficient,Sparsity-Preserving,Online-Algorithm For Data Approximation》

吸引力指数: 3

这是一篇数值优化的文章，提出了一个算法**SRLU**,是LU分解算法的一个variants。据作者描述，该算法有诸多好的性质，比如高效，准确，稀疏性保持，保持重要数据特征的能力，此外文中有大量详细的数学证明。

4.《Lazifying Conditional Gradient Algorithms》

对Frank-Wolfe方法(条件梯度算法)的改进，在对该方法**Lazifying**后，计算时间明显降低。这种文章个人感觉很数值优化，但是发在了ICML上，在abstract中作者说这种方法在Online Learning中用到。

5.《Conditional Accelerated Lazy Stochastic Gradient Descent》

这篇文章和上篇文章的作者基本保持一致，同一个会两篇内容类似的文章，囧。文章中作者得到了一个更好的界比起基于随机梯度下降的Online Frank-Wolfe方法。

#### 高性能

1.《Deep Tensor Convolution on MultiCores》

文章基于CPU的上下文，提出了更快的卷积方案。卷积和互信息之间有没有关系？如果有，可否将卷积用于图像配准？

2.《Efficient Softmax Approximation For GPUs》

Facebook AI Research的文章，提出了高效训练基于语言模型的神经网络的方法。这种方法称为**Adaptive Softmax**，insights在于模型架构的优化和矩阵向量间操作的优化，使之更好的发挥GPU的计算能力。

3.《Fast k-Nearest Neighbour Search via Prioritized DCI》

采用动态连续索引改进K近邻在大数据环境下的搜索。

#### 其他

1.《Developing Bug-Free Machine Learning Systems With Formal Mathematics》

刚开始没看的太明白。

2.《Dance Dance Convolution》

作者基于语言模型开发了舞蹈学习的游戏。


**参考:**

0.[ICML2017_arXiv](https://github.com/xiaojudou/ICML2017_arXiv)

Accepted list刚放出，有人写了一个爬虫爬到了arXiv上已经放出的文章。因为官方网站还不提供下载，注意这只是预印版，如果要精读文章，最好还是官网下载（我都是人肉从arXiv上下载）。

1.[ICML accepted papers institution stats](https://medium.com/@karpathy/icml-accepted-papers-institution-stats-bad8d2943f5d)

Andrej Karpathy做的一个stats，关于论文机构，关于学术界和工业界的对比，可以看到工业界虽然发挥了很大的贡献(部分顶级工业界实验室文章频数高)，但是学术界依然是主力(主要集中在长尾)。

2.[ICML@Sydney](https://2017.icml.cc/Conferences/2017/AcceptedPapersInitial)

论文接受列表

3.[让计算机学会学习Let Computers Learn to Learn](https://zhuanlan.zhihu.com/p/21362413)

DeepMind ICML 2017的文章 Learning to learn without gradient descent by gradient descent

4.[关于K-MC^2的讨论](https://zhuanlan.zhihu.com/p/25037146)
