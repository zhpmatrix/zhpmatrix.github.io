---
layout: post
title: "GBDT-下里巴人版"
tags: [数据挖掘]
excerpt: "XGBoost是GBDT的高效实现，本文用一个简单的例子说明GBDT工作方式，尽量不堆公式。同时关于XGBOOST，给出了几个经典的问题，关于XGBOOST在实现上可能带有作者们自己的偏好，不管怎样，从比赛和项目使用来看，效果很不错。"
date: 2017-04-01 20:00:00
---

给定模型GBDT，记为G。给定训练集，记为D。训练集如下(假设为用户一年内的统计数据)：

|用户ID|天猫购物金额|在百度知道提问次数|年龄|
|--|:---:|:---:|---:|
|A|200|20|14|
|B|300|18|16|
|C|1500|10|20|
|D|800|8|24|
|E|3000|28|26|

给定测试集，记为T。测试集如下(假设同训练集)：

|用户ID|天猫购物金额|在百度知道提问次数|年龄|
|--|:---:|:---:|---:|
|F|4000|2|?|
|H|100|32|?|

此处声明：数据均为假设数据，其实我也不了解不同年龄段的同学们的消费和提问情况。

问题定义如下：

    在给定模型G和训练集D的前提下，训练模型G，利用训练后的模型G预测测试集T的年龄。

首先连续数据离散化，对D和T的离散化结果如下(购物金额阈值=1000，提问次数阈值=20)：

训练集:

|用户ID|购物金额<=1000|提问次数 <= 20|年龄|
|--|:---:|:---:|---:|
|A|1|0|14|
|B|1|1|16|
|C|0|1|20|
|D|1|1|24|
|E|0|0|26|

测试集:

|用户ID|购物金额<=1000|提问次数 <= 20|年龄|
|--|:---:|:---:|---:|
|F|0|1|?|
|H|1|0|?|

让我们先看看传统的回归树的做法:

![回归树](http://wx2.sinaimg.cn/mw690/aba7d18bgy1fe7gai1lqrj20gw08caa4.jpg)

模型训练完成，预测F和H的年龄：

依据F和H的属性，按照建立的树结构，用F和H落入的叶子节点的值作为F和H的预测年龄值，则F和H的年龄分别为20和20，虽然预测年龄相同，但是按照建立的模型，二者落入了不同的叶子节点。

让我们看看GBDT是怎样做的吧？模型G假定建立两棵树，每棵树的最大分支数为2。如下:

![GBDT](http://wx4.sinaimg.cn/mw690/aba7d18bgy1fe7gxvo95ej20cq0dc3yn.jpg)

【修正】红色残差值应为: B=-7/3,C=-10/3,D=17/3,A=-7/2,E=7/2

模型训练完成，预测F和H的年龄：

对于F:
    购物金额>1000，按照第一棵树，预测年龄为23，记为F_AGE_1 = 23
    提问次数<=20，按照第二棵树，预测年龄为1/3，记为F_AGE_2 = 1/3

故F的预测年龄为：F_AGE = F_AGE_1 + F_AGE_2 = 23 + 1/3 = 23.33

对于H，预测方法同F。

对于上述的例子，可以帮助我们建立GBDT的直觉，关于GBDT的理论部分，还有很多有意思的内容可以进一步的挖掘。

**几个典型问题：**

1.基本概念：GBDT(Gradient Boosting Decision Tree)的别名MART(Multiple Additive Regression Tree)，GBDT中的树都是回归树，或者说GBDT中的基学习器都是回归树。

2.选择分割属性和分割阈值的目标：C4.5中谈到信息增益(找最大)和基尼系数(找最小)，而回归树中是均方差(找最小)，分枝直到每个叶子节点中的label(数值)相同或者达到其他预设的终止条件，例如叶子个数的上限。

3.Gradient Boosting中的Gradient是来自**均方差求导，得到残差**。用残差作为全局最优的绝对方向，不需要用到求导的Gradient。

4.Adaboost通过给错分样本加权和Bootstrap，可以有效防止overfitting，但是没有理论上证明。有些GBDT的实现添加了Re-sampling，但是**由于随机性的引入很难证明当出现一个好的结果时，是因为选择了好的特征，还是因为随机性**。

5.**Shrinkage**为每棵树设置了一个weight，累加时乘这个weight。该参数也是XGBOOST中的learning_rate或者是eta参数，一般和num_round或者n_estimators配合使用。


参考:

1.[GBDT(MART)迭代决策树入门教程](http://blog.csdn.net/w28971023/article/details/8240756)

文中的例子是对参考文章中例子的一般改进，使得数据看起来不那么完美才接近真实，同时给出了连续数据离散化的过程，这样一个标准的训练过程就有了。参考文献中在回归树部分使用了不同的属性，给出了作者的一个对比想法，作者同时提出的几个问题也是帮助我们建立直觉的很好的方式，强烈推荐这篇文章。

2.[Notes on Parameter Tuning](http://xgboost.readthedocs.io/en/latest/how_to/param_tuning.html)

这是XGBOOST的官方tuning文档，从bias-variance均衡到控制overfitting到imbalance的处理，其中提到一个有意思的形容词**conservative**，表示对noise不敏感，不容易过拟合。






    