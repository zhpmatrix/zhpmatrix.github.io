---
layout: post
title: "[NLG]论文阅读-《SeqGAN:Sequence Generative Adversarial Nets with Policy Gradient》"
excerpt: "NLG的几个思路：GAN，VAE，RL，其他和流模型，之所以将流模型放在最后，是因为目前还没有看到流模型用于NLP的任务。"
date: 2019-02-04 18:00:00
mathjax: true
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

主要思想：将生成模型看做RL中的agent，至今已经生成的token看做state，将要产生的下一个token看做action，利用一个判别器评估句子得到reward，将生成模型同时看做随机参数化的policy，用蒙特卡罗搜索近似state-action的值，利用policy gradient去训练policy，也就是生成模型。

作者在文章中的描述如下：SeqGAN extends GANs with the RL-based generator to solve the sequence generation problem, where a reward signal is provided by the discriminator at the end of each episode via Monte Carlo approach, and the generator picks the action and learns the policy using estimated overall rewards.

模型架构如下：

![SeqGAN](http://wx1.sinaimg.cn/mw690/aba7d18bly1fztmu8iftuj20gi06jjsn.jpg)

左侧是判别器(D)，用于区分真实数据和由生成器(G)生成的数据，是二分类器。右侧是SeqGAN的关键部分，对于生成的部分序列，通过蒙特卡罗搜索(MC Search)组成完整序列，将组成后的完整序列作为D的输入获取Reward，通过Policy Gradient更新中间Action值，继而实现G的训练。具体模型设计上，generator是RNN系，discriminator是CNN。由于文本是逐字符生成的，而D的输入是整个序列，因此为了得到每个词的reward，每得到一个新的生成词，就结合之前生成的文本序列作为整句，得到的reward当做当前生成词的reward。显然这样相较于依据于整句直接得到reward，合理了许多。在SeqGAN中，是通过MC Search实现的，而在下文中将要提到的Conditional SeqGAN中，是通过REGS(Reward For Every Generation)实现的。

MC Search的过程是这样的。假设已经生成了部分序列，则基于G，固定部分序列作为前缀，重复生成多个完整序列，将多个完整序列作为D的输入，得到reward的平均作为整体reward的估计。这样带来的问题分别是计算开销大和过拟合(后期部分序列相较于前期部分序列)。

上述过程主要讨论D需要完整输入的情况。序列是逐个字符生成的，中间状态存在部分子序列的问题。而D需要完整的输入，因此必须通过一个方法去构造一个完整序列，这个方法就是MC Search。但是，能不能不要构造完整序列呢？直接将生成的部分子序列作为D的输入。

答案是可以，但是显然这种方法加快了速度，但是减少了精确度。典型的以精度换时间。

要知道，原始GAN在面对离散型数据的时候，D无法将梯度BP给G。本质原因在于损失函数中的JS散度度量。由于文本任务的离散性质，导致原始数据分布和生成数据分布不重叠，JS散度为常量，也就是D丧失学习的意义。这里从数值角度出发，关键在于softmax后得到的vocab大小的概率向量要不要转化为one-hot向量，进而得到对应字典值的问题。针对上述问题，两个典型的工作分别是WassersteinGAN和Gumbel-Softmax。前者用到Wasserstein距离(推土机距离)，解决JS散度的问题。生成数据的概率向量可以不转化为one-hot向量和真实数据对应的one-hot喂给D进行判别。后者则从Softmax出发，寻找一个更好的"Softmax"，使得既不会带来数据分布不重叠的问题，同时避免遇到离散型数据的不可微分问题。

WassersteinGAN是找到一个新的技术直接替代原始的技术；Gumbel-Softmax是使用一个更加通用的Softmax解决原始Softmax遇到的问题。这两种想法个人都是非常喜欢。

SeqGAN的思路则不同以上，借助RL和GAN的思路，避开了离散数据的BP难题。SeqGAN的工作是重要的，是第一次将GAN用于离散文本序列。由此，引发了后续的Conditonal SeqGAN，二者的区别如下(此图找不到来源了)：

![img2](http://wx1.sinaimg.cn/mw690/aba7d18bly1fztp0gbh6zj21220aojvf.jpg)

Conditional SeqGAN的经典使用是对话生成和文本到图片任务。

回顾上述过程，RL可以单独用于对话生成，但是需要人工作为reward的来源。一种可行的方式是提前训练好一个模型作为"人工"用于产生reward，但是该"人工"是静态的。而SeqGAN中借助D作为reward的来源，同时基于GAN的框架来分别训练G和D，这里的"人工"是动态的。从这个角度来看，D是度量的学习器，而度量并不需要一定显式地定义。不严格地说，在NN的世界，所有的事物都是函数，所有的行为都是函数拟合。

在上文中讨论了reward的获取，实际上raward的估计模型本身也是可以被学习的，刚好印证了上述观点。这就是Actor-Critic，其中Actor基于policy gradient优化策略，Critic借助Q-Learning学习action-value值函数。因此，从这个角度也可以认为GAN是一种特殊的Actor-Critic。

值得一提的是，在文章中，提到了一个评估指标NLL(oracle)，其中oracle是一个随机初始化的LSTM，简记为\(G_{oracle}\)，然后用\(G_{oracle}\)生成训练数据训练生成模型\(G_{\theta}\)。训练结束后，用\(G_{\theta}\)随机产生测试数据，用\(G_{oracle}\)检测\(G_{\theta}\)生成的测试数据是否符合自己的分布。

在[Texygen平台](https://github.com/geek-ai/Texygen)中，给出了另外一个度量指标NLL(test)和上述指标形成对偶关系。具体表达如下：

$$NLL_{oracle}=-E_{Y_{1:T \sim G_{\theta}}}\left[ \sum_{t=1}^{T}log(G_{oracle}(y_{t}|Y_{1:t-1}))\right]$$

$$NLL_{test}=-E_{Y_{1:T \sim G_{oracle}}}\left[ \sum_{t=1}^{T}log(G_{\theta}(y_{t}|Y_{1:t-1}))\right]$$


从整体上看，GAN可以作为无监督文本生成的模型，而且具有极大的灵活性。LSTM之父和Ian之间没有太大争辩的必要性。而该模型利用无监督文本的方式很特别，通过极度relax的方式构建标签用于判别器D的学习，同时D的输入是另外一个模型G的产出，最终期望生成数据分布和真实数据分布具有一致性。同时从文章来看，SeqGAN的训练似乎并不是一件容易的事情。

Ian在AAAI2019的invited talk中提到：Regarding leveraging GANs for NLP, we currently have not found a good way to deal with the large action space required to generate sentences with RL.


参考资源

1.[GAN在自然语言处理方面有哪些有意思的文章和应用？](https://www.zhihu.com/question/54463527)

2.《Neural Text Generation：Past，Present and Beyond》

3.[Role of RL in Text Generation by GAN](https://zhuanlan.zhihu.com/p/29168803)

4.[SeqGAN的代码解析](https://zhuanlan.zhihu.com/p/44729684)

5.《Policy Gradient Methods for Reinforcement Learning with Function Approximation》

6.《Texygen: A Benchmarking Platform for Text Generation Models》

7.GAN的损失函数构造相关，[一般情况](https://zhuanlan.zhihu.com/p/26994666)，[特殊表示](https://kexue.fm/archives/4439)
















