---
layout: post
title: "[NLP]关于Continual Learning在NLP问题中的应用讨论"
excerpt: "业务多变，模型要连续迭代，CL就是一个工业界非常实际的问题。CL@NLP的工作相对较少，这篇博客主要做一些围绕这个问题的思考，为后续研究做理论准备。"
date: 2020-02-09 10:25:00
mathjax: true
---

**前言：**

从术语上，一般认为Life Long Learning(LLL) = Continuous Learning = Never Ending Learning = Incremental Learning。从近期梳理的一些工作来看，学术界围绕CV领域的Recognition和RL相关的问题研究工作较多，但是直接针对NLP的问题做的工作非常少，但是从已经有的工作来看，理论上是可以做在NLP的问题上的。

个人觉得整体的理论体系梳理的尚不够清楚，需要后续结合实践上的理解，进一步完善。

### 问题设定

#### 1.命名实体识别问题

假设现在有一个支持三种类型（人名，地名，爱好）的序列标注模型A，则对一个句子，如下：

> 老吴，生于安徽，爱写码，不爱钓鱼，就职于宇宙最强科技有限公司。

有预测的结果，如下：

+ （老吴，人名）
+ （安徽，地名）
+ （写码，爱好）

这里的“钓鱼”可能相对容易被误标为“爱好”，因为要结合上下文理解到“不”的否定含义，暂且不谈。

当需要模型支持对新类型“公司”的预测，得到模型B，也就是希望现有模型的标注结果中，新增：

+ （宇宙最强科技有限公司，公司）

怎么办？实际上，对于一个实体识别模型的期望是：横向上能够识别更多的字段，纵向上每个字段能够做到很棒。

#### 2.分类问题

模型A能够做四分类，现在想要增加一个新的分类类别，得到模型B。**能够借助于“其他类”实现吗？**

#### 3.纠错问题

模型A能够做六种子问题的检纠错，现在想要增加一种新的子问题（形近字），得到模型B。

#### 4.多标签分类

标签个数不定，随着业务变化，逐渐增长。

#### 5.模型的hotfix

在生产环境下，hotfix的手段包括写规则，后处理，别的方案共同预测，增强类似badcase后重新训练等，但是这些都不是优雅的方案。hotfix需要基于原始模型fix一个badcase，同时不影响其他case。

#### 6.其他

《Three scenarios for continual learning》这篇文章总结了三种场景，如下：

+	基于task的：需要做新的任务，体现在多个输出端，但是有共享层
+  	基于domain的：任务的结构相同，但是输入分布不同
+   基于class的：需要学习新的类

个人并不认为这是个很好的分类方法。

### 方案讨论

**1.重新标注数据A，标注支持四种类型（人名，地名，爱好，公司），然后重新训练得到模型B。**

对于数据准备，需要基于一份数据，两次标注。第一次标注前三种类型，第二次需要在第一次的基础上标注一个类型（公司）。需要具体思考的问题是是，补标数据和已标数据是否是一致？如果一致，重新标注新类型即可。如果不一致呢？也就是说数据A中不包含新类型。

这种情况下，首先需要标注了新类型的数据B，其次模型B至少要用到数据B。问题是，模型B如何保持对原始三种类型的预测能力？

第一种：在数据B上标注四个字段。这种方法存在明显的效率低下的问题（标注成本高，数据利用率低等）。可以讨论的问题是：

+ 当数据B中有旧类型字段时，那么可以用模型A去预测旧类型字段，人工标注新类型字段。本质上是一个Weak Supervised Learning问题。

第二种：在数据B上标注新字段，合理利用数据A和模型A。这种方法看起来相对理想，问题是如何利用？可以讨论的问题是：
 
 + 数据B只标新类型，然后合并数据B+数据A。需要思考的是，虽然数据A中不包含新类型字段，但是数据B中可能包含旧类型字段，由于数据B中没有显式标注，当基于合并后的数据训练模型时，这会confuse模型B对于旧类型的learning，这一定程度上可能需要PU Learning的帮助。当然，数据B中不包含旧类型时，数据A和数据B在类型上是正交的，数据上没有问题，直接合并即可。

对于模型训练，可以train from scratch，也可以将模型A作为pre-trained的模型。

讨论了上述问题之后，可能比较有价值的问题是：

**给定模型A和只标注了新字段的数据B，得到模型B，对于四种类型的字段有预测能力。**

**2.其他方案（连续学习）**

为了实现我们的目标，方案可能需要具有的特点：

+ 学过的知识要能够记住，但是允许少量的遗忘（解决存量问题）
+ 要能够做知识迁移（解决增量问题）

嗯，**multi-task learning或许是一种潜在的方法。**但是multi-task的方式也许会遇上存储和计算的问题，导致该方案并不是特别的fancy。围绕存储问题，一种解决思想是：

在训练任务A的时候，同时训练一个数据A的生成器G。训练任务B的时候,合并**数据B**和**任务A的生成器G**。本质上解决的是从存储数据自身，改为存储一种数据生成的机制。那么问题来了，怎么保证生成器的效果？

除此之外，模型学习到的内容以参数形式存储。所谓要不退化，可以理解为某些参数不能被大幅度修改，如果违反该条件，模型能力在旧任务上就会形成退化。因此，对应的思想是：

对重要参数添加约束，该约束表明了对应参数的重要程度。这里的一个问题是，假设任务A和任务B对应的重要的参数一样重要呢？任务B希望更新重要参数，以提升对数据B的拟合能力。而任务A不希望更新重要参数，以保证对数据A的预测能力。比如，两个任务，同一种网络结构。

针对知识迁移这个问题，连续学习要求模型在任务A学到的内容能够有助于任务B的学习，同时不能忘记在任务A中学到的知识；而迁移学习只关心前者，实际上多数情况下，迁移方法会导致一定程度的退化现象。

**3.增加一种新的类别**

+ 《Learning without Forgetting》(损失函数端的工作)

+ 《iCaRL- Incremental Classifier and Representation Learning》（数据端的工作）

+ 《Incremental Learning with Unlabeled Data in the Wild》（损失函数端+数据端）

**4.如何评估连续学习的效果**

+ 准确率

+ 前向迁移效果

+ 后向迁移效果

**5.模型扩张和任务扩张**

当模型容量足够大的时候，一个模型可以学习所有任务；但是当模型容量不够的时候，在任务扩张的时候要做模型容量的扩张。显然，这里的目标是：

模型容量扩张的速度小于任务扩张的速度。

**6.学习的顺序**

学习的顺序是重要的，这是课程学习的研究内容。

### 实验设计

**1.数据集**

多分类数据集：[今日头条新闻分类数据集](https://github.com/skdjfla/toutiao-text-classfication-dataset)，共15类。

序列标注数据集：OntoNotes-Release-5.0，中文共18个标签。

多标签（>34个标签）和纠错的数据集(7个子任务)均是来自自有业务数据。



相关阅读：

1.[知乎](https://zhuanlan.zhihu.com/p/68421371?utm_source=qq&utm_medium=social&utm_oi=52727124066304)

2.[awesome-incremental-learning](https://github.com/xialeiliu/Awesome-Incremental-Learning)

3.[深度学习微调网络,会导致网络对原本的数据识别效果变差吗？](https://www.zhihu.com/question/360374828)

4.[深度学习中如何避免灾难性遗忘？](https://zhuanlan.zhihu.com/p/29196822)


总体上该方向包括三种解决问题的角度：数据端，模型参数端和损失函数端。这和多数情况下DL中的方法论是保持一致的。针对数据端，这篇文章中提出的一个观点：

> 保留少量代表性的样本来解决Catastrophic Forgetting应该是一个很有意义的方向，但是这个方法仍然做出了太多过强的假设，应当后续有不少发展和改进的空间。

围绕该问题的研究从[1989年](https://www.sciencedirect.com/science/article/pii/S0079742108605368)就开始啦。

5.[Introduction to Pseudo-Labelling : A Semi-Supervised learning technique](https://www.analyticsvidhya.com/blog/2017/09/pseudo-labelling-semi-supervised-learning-technique/)

6.《One Model to Recognize Them All: Marginal Distillation from NER Models with Different Tag Sets》