---
layout: post
title: "MT-DNN相关"
tags: [NLP]
excerpt: "论文阅读"
date: 2019-08-08 15:48:00
mathjax: true
---

1.《Multi-Task Deep Neural Networks for Natural Language Understanding》

这篇文章是刘晓东2019年的工作，是基于下一篇文章，也就是2015年的工作做的。第三篇是基于这篇做的，同样是2019的工作。

两种在NLU中获得representation的方法：

（1）多任务

（2）预训练语言模型

**个人想法：**在当下，显然（2）更加具有潜力，从最近的一些工作中也可以看到。从训练数据角度，多任务学习需要不同任务下的监督数据，而预训练语言模型只需要文本，也就是说，前者仍旧会受到数据规模的限制，很难在短期内做很大的模型，后者就可以很容易做到。从训练方式角度，前者需要任务设计和合理的训练策略，后者只需要训练一个语言模型和较之前者非常简单的任务设计，简单粗暴拼算力，只要加卡就能解决的问题，可操作性更强。

当然，以多任务的方式训练得到的模型，可以继续做fine-tuning，也可以直接用于多个任务中的某一个；预训练语言模型则通常需要fine-tuning。因此，从对模型的使用方式上来看，也并不具备很多优势。

但是多任务自有其优点，如下：

**多任务的优点：**

（1）可以利用更多的数据。

（2）正则化。不同任务可以防止模型overfit到某一特定任务。

预训练语言模型本质上仍旧是单任务的。但是由于对“任务”的理解不同，可能有些多任务仍旧是某种意义上的单任务。

**什么是多任务？**

文章给出的一个多任务的例子设计：单个句子的分类任务（情感分类），两个句子的分类任务（文本蕴含），文本相似性评分（语义相似性）和相关性排序任务（给定query和候选answer列表，判断候选answer中是否存在正确answer）。

**MT-DNN的架构是怎样的？**

共享层(Transformer)+特定任务层(用于四个不同的任务)。

**如何训练？**

将所有task的数据分别pack成mini-batch，合并后shuffle。每个mini-batch对应一个task的数据，只更新对应task的weight。

**后续可以继续的工作？**

（1）更高效的可以利用任务相关性的训练方法。包括在pre-training阶段和fine-tuning阶段。

（2）用显式且可控的方式添加语言学知识。

（3）MT-DNN是否具有更好的预防对抗攻击的能力。

2.《Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classiﬁcation and Information Retrieval》

这篇文章是刘晓东2015年的工作。整体上的思路同上，不过比较有意思的内容在于训练结构的设计。比如这种：

![img](http://wx3.sinaimg.cn/mw690/aba7d18bly1g5scd008qtj20k507w0t0.jpg)

Q和D可以看作一个问答匹配的任务，但是不把该任务当作多任务中的子任务，只把Q作为多任务训练的一部分，D是单独一个网络结构。**结合任务的多样性，相信有更多的架构设计方式**。

3.《Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding》

这篇工作是刘晓东在第一篇工作的基础上做的。主题是多任务场景下的知识蒸馏。个人不是很喜欢，只提及一下。

相关工作：

1.[Multi-Task Learning for Sentence Embeddings](https://medium.com/@makcedward/multi-task-learning-for-sentence-embeddings-55f47be1610a)

2.《An Overview of Multi-Task Learning in Deep Neural Networks》

写了这篇博客之后才看到这篇文章，其实可以先看这篇文章的。
