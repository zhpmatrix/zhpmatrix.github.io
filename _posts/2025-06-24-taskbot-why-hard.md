---
layout: post
title: "为什么构建一个财税对话系统是难的？"
tags: [对话系统]
excerpt: "垂直强知识依赖的对话系统都是极具挑战性的工作"
mathjax: true
date: 2025-06-24 11:00:00
---

围绕构建财税咨询对话的可能技术路线包括专家系统，RAG(传统QA库可以是RAG的一个具体知识组织形态，更高级的Agent)和模型微调。RAG的核心在于知识而非技术，当然可以在技术侧通过模型微调等方式形成适配领域的基础模型能力。这种条件下，如何构建一个符号系统(可能是专家系统)来形成技术壁垒可能就是一个重要问题。当然这里未必是清一色的符号解决方案，实际是符号和连接在狭义上的整合。
这样的系统是否在效果上一定优于RAG？
这篇文章试图解释为什么构建一个以符号为主的财税对话系统是困难的？同时讨论可能的其他方案。

#### 模型知识 v.s. 人类知识

借鉴心理学的[乔哈里视窗模型](https://zh.wikipedia.org/wiki/%E5%91%A8%E5%93%88%E9%87%8C%E7%AA%97)，围绕模型和人类对知识的知道和不知道的情况，总共存在四种情况，分别如下：
+ 模型知道+人类知道
+ 模型知道+人类不知道
+ 模型不知道+人类知道
+ 模型不知道+人类不知道
实际上，对于强知识依赖的场景，真正的难点在于：人类并不知道模型知不知道。如果事实成立，意味着人类知识和模型知识的边界是不清晰的。这样的话，为了提升模型的效果，主观判断需要增加知识的时候，人类并非清楚地知道应该补充什么知识。
此外，在文章《Inside-Out: Hidden Factual Knowledge in LLMs》中提到，模型表现出来的知识和模型实际知道的知识有40%的差距。而我们对于模型是否掌握知识的判断多数情况下是通过针对模型表现出的行为进行识别的。这种滞后性使得人类对于模型掌握知识的判断进一步困难。

对于一个专家系统，知识是以明文的方式在表示。这里的知识是狭义的表达，那么似乎这里更本质的思考问题应该是：什么是知识？

对于一个要求高准确的咨询对话系统，这会带来什么问题？比如在要素抽取任务中，会发现模型表现出的行为好像是不知道某个具体知识，比如免税发票的含义，但是单独询问大模型免税发票的含义，大模型的回复是正确的。这个时候是否应该添加业务专家知识？如果添加，应该怎样添加知识？

#### 苦涩的教训

强化学习Rich Sutton在2019年的文章《The Bitter Lesson》中谈到如下:

**The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. **

一些研究者试图将领域知识注入到他们自己的agent中，这种方式通常在短期内是有效的，同时能够个性化地令研究者感觉非常开心，但是长期以来就会因为遇到瓶颈从而阻碍进一步地发展，直到通过搜索和学习的方式进行可扩展的计算。

财税咨询作为一个类似医疗诊断的严肃对话场景，为了尽可能的降低资损问题，需要的方案满足高准确，可追溯的特点。这种约束条件下的方案多数情况下是满足（1）和（2）的，这也意味着大概率会遇到（3），至于能否以及什么时候遇到(4)是未知的。

LLM的成功进一步地在验证Sutton的观点。

#### 不确定性推理（不精确推理）

不确定性几乎是财税咨询对话系统推理的复杂度和难度的唯一原因。

坐席基于不确定性知识对访客提供的不确定性信息进行不确定假设，经过反问进行不确定性信息收集，对不确定假设进行不确定推断，最后给到一个不确定但合理的咨询结论。

以不确定性知识为例，其中不确定性知识包括如下：
+ 概率性知识。概率性是指事件在发生之前，其结果是不确定的，但是事件已经发生，则结果是明确的。
+ 模糊性知识。模糊性是指即便事件已经发生，但是结果也无法用二值逻辑进行判断，例如头疼程度。
+ 经验型知识。经验型知识都带有一定的可信度，结论可能正确，也可能不正确，更多时候是部分正确。
+ 不完全型知识。对某事物还不完全了解，或者认识不够完整和深入的时候，就会产生很多不完全的知识。这些知识通常是部分正确，或者结论的覆盖范围很大，不能精确地限定两个事物间的联系。

事实和知识是构成推理的两个基本要素。已知的事实是证据，用以指出推理的出发点和推理过程中所使用的知识，知识是保证推理前进并逐步达到目标的依据。

不确定性推理要解决的关键问题如下：
+ 不确定性的表示和度量
+ 不确定性的匹配
+ 不确定性的传递算法
+ 不确定性的合成

比如对于面向要素的推理规则而言，每个规则存在一个推理强度的描述，形式化的表达方式为(if...then..,推理强度)。基于抽取的要素和推理规则能够得到最终用于规则召回的要素，召回阶段利用含有不确定性度量的要素进行规则筛选，基于筛选的规则在下游组合答案的时候，对于规则中不确定性的要素，会采用特殊的模版来表达，比如假设发票类型是不确定性要素，组装答案的模版会采用“如果...那么...”类似的表达方式，对于确定性要素，则在答案呈现的时候将要素作为既定事实来使用。

#### 交互

在[要素抽取之我见](https://zhpmatrix.github.io/2025/06/24/taskbot-entity-extractor/)中谈到要素抽取的动态性，其中谈到通过反问的方式获取证据。在咨询对话场景中，是因为访客不知道问题和答案是什么，所以才来人工咨询。如果访客知道问题，访客可能都会采用其他渠道来解决问题。所以过程中，访客会试探性的问一个问题，通过观察坐席的反问来明确自己的真正问题是什么，进而实现访客和坐席的同频。

也就是说，反问是访客和坐席明确对方意图的重要动作。坐席通过反问明确访客的问题(意图)，访客通过反问明确坐席的回答。过程中，可能访客是知道问题，但是不知道答案的，同时对于坐席的回答，多数情况下似乎缺乏证伪的能力。从这个角度来看，专业咨询和医疗诊断存在一定程度上的类似性。

#### 有没有其他可能

构建财税咨询对话的目的是给定访客问题，能够给出一个适合的答案。而在系统中存在非常多的咨询对话记录，那么能否利用这些咨询对话记录直接通过模型微调的方式构建一个对话系统？其中存在的问题如下：
+ 财税咨询以财税政策为核心，但是财税政策是有时效性的，但是历史咨询对话是静态的。导致访客的问题和坐席的回答均是有时效性的，如何筛选不符合最新政策规定的数据？
+ 针对访客的问题，坐席的回答是不正确的。这部分数据也需要去掉？怎样识别到这种不正确？
+ 财税咨询是一个强推理系统。大前提是政策，小前提是访客的具体场景，答案是针对具体场景的回答。单纯的从咨询对话中学习，能否学到政策的信息？模型的泛化能力是指什么？
+ 财税咨询场景是否需要一个世界模型？能够理解农产品都有哪些？皮毛是什么？深加工又是什么？

由于上述问题的存在，是否知识侧必然是以明文存在而非参数化？

笔者认为大概率是的。如果符号系统的路线本质上是构建一个不确定性推理系统，那么不确定性推理系统的天然缺陷意味着这条线大概率是无法持续推进的，因此最终会收敛到RAG，但是这里的RAG并非传统意义上的RAG，是一个能够scale的Agentic的系统能力，类似Sutton所言，需要search和learning的能力。

**[扫码加笔者好友](https://zhpmatrix.github.io/about/)，茶已备好，等你来聊~**
