---
layout: post
title: "LeNet"
tags: [深度学习]
excerpt: "文章通过梳理一个具体的网络结构LeNet，计算参数数量和连接数，分析卷积核和下采样核等，感性的认识网络结构的建立，同时给出了自己的判断。"
date: 2017-06-27 14:59:00
mathjax: true
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

前言，本文从[文章](http://blog.csdn.net/strint/article/details/44163869)盗图不少，此外从顾险峰老师的文章《人工智能中的联结主义和符号主义》中盗了一些图，类似文章有很多，本文只是帮助自己去梳理网络结构的记录，在梳理的过程中感受这个结构，才是本文的目的，同时**总结**部分是自己的认识。

---

![LeNet-5](http://wx1.sinaimg.cn/mw690/aba7d18bgy1fgyq2wtgp6j20qw08y769.jpg)

---

LeNet-5的连接方式包括：卷积层，下采样层和全连接层。卷积层做的事情如下所示：

![Conv](http://img.blog.csdn.net/20150309234105633)

比如对于第一个卷积层，输入是**边缘填充过的图片**(_填充方法有多种_)，输出是feature maps，每一个feature map对应上图中的下一层节点矩阵，**连接的参数矩阵是卷积核**，每个feature map使用相同的卷积核，称为**共享参数**(_注意每个卷积核对应一个bias_)。将卷积结果加上bias送入激活函数，结果作为feature map的一个值。

下采样层的输入是feature maps，输出是下采样后的maps。具体的采样方式是用一个2x2的下抽样核，每次下抽样核移动两个像素，对应输入域(核)内元素求和后取平均，乘上一个系数后加上一个bias送入激活函数。

从上述过程来看，卷积层和下采样层在操作层面是类似的。不同的地方在于卷积核的每个参数通常不同(取决于训练结果)，下采样层的参数都相同(或者说只有一个参数)。

C3和S2的连接方式很特别，C3的每个feature map和S2的多个map连接，具体连接方式如下：

![C3&S2](http://img.blog.csdn.net/20150309234221653)

其中C3中的map0的连接方式如下：

![map0](http://img.blog.csdn.net/20150309234350245)

注意，上述卷积和的大小还是5x5，通常**不对称的组合连接的方式有利于提取多种组合特征**，在数据挖掘比赛中，特征工程中的一个重要部分也是对组合特征的使用。

S4层的图的大小是5x5，和卷积核大小相同，卷积后形成的图的大小为1x1，也就是C5层的图，LeNet-5形成120个卷积结果，连接方式如下：

![C5](http://img.blog.csdn.net/20150309234409869)

LeNet-5的完整结构**训练参数有60840**，**连接数340908**。完整数字识别效果图如下：

![Digital Recognition Fig](http://img.blog.csdn.net/20150309234530724)

**高层特征是低层特征的组合，从低层到高层的特征表达越来越抽象和概念化，也即越来越能表现语义和意图。**

下图是顾险峰老师在文章《人工智能中的联结主义和符号主义》中提到的深度神经网络在人脸识别任务中不同层学习到的人脸，依次是从低层到中层到高层。

![face vis](http://photocdn.sohu.com/20160316/mp63710466_1458109843738_5.jpeg)

在李飞飞老师的CS231n课程中，绪论讲述了人类视觉系统的研究，人类视觉系统在识别时，首先识别到边缘，然后到形状。从数字识别的效果图中可以看出，在特征表达层面越来越抽象，同时距离语义也越来越近。

**总结：**

本文较为详细的梳理了一个网络结构LeNet-5，相关著名的网络结构很多，在[啊哈，CNN!](https://zhpmatrix.github.io/2017/06/01/something-about-dl/)文章中提到的文献有详细描述。在CS231n中，讲师们说**做网络模型，就像搭积木**。然而，每个人都可以做自己的结构，但是**在benchmark上能够work的模型却非常有限**。这句话包含几个意思，第一，目前相当大一部分结构是在MNIST,CIFAR10,CIFAR100等数据集上进行评测，但是放到真实世界，泛化能力怎样需要进行评估？第二，即使在benchmark上，也只有很少的一部分模型能够work，例如通过ImageNet等这种比赛产生的网络结构。之前看过一些深度学习中的优化算法的文章，其实与**深度**关系不太大了，文章的证明是很少甚至没有，但是直觉上的表述有很多，实验结果证明在精度和速度上的提升，这已经够了。因此，**个人很不认同深度学习是黑盒**这样的观点，这个是实验方法，同样也是科学，效果足够好就可以，个人认为**追求解释性一部分是为了提高可控性**，可是如果足够稳定，适当的弱解释性也是可以接受的。见过有些同学，直接深度学习，目前来看，这是可行的。但是从长远来看，读书的时候多花点时间在数学上应该不会错。上述观点仅仅是基于CNN的一些内容作出的一个不严格的感性的判断，不管怎样，萝卜白菜，各取所爱，不应对实验科学，对深度学习保留偏见(_希望这段不被老板看到_)。




参考：

1.[神经网络训练中的梯度消失和梯度爆炸](https://github.com/wangminhang/CarND-LeNet-Lab)

问题的本质是sigmoid激活函数，导致的原因是误差反向传播过程中的连乘。Bengio在文章《Understanding the difﬁculty of training deep feedforward neural networks》中研究了不同的激活函数在深度训练过程中的表现。

2.《Gradient-Based Learning Applied to Document Recognition》

描述LeNet-5的原始文章，其中对该网络结构的描述比一般看到的文章要详细的多。



















