---
layout: post
title: "Neural Networks Theory"
tags: [深度学习]
excerpt: "深度神经网络的损失函数的非凸性讨论，局部最小值和鞍点理论，如何用可视化的方式讨论神经网络的损失函数"
date: 2017-12-31 14:59:00
mathjax: true
---

深度神经网络可解释的问题一直是一个研究热点和难点，可以翻COLT，NIPS，ICML等会议文章。一般会从三个角度来讨论：表达能力，优化难度，泛化。站在表达能力角度，一般认为网络更深，表达能力会更强一些。要简化优化难度，需要对数据和优化问题做假设。在假定数据和优化问题满足假设条件的前提下，使用SGD及其变种进行优化，也可以得到一个不错的解。针对泛化的讨论目前的一个焦点是Flat minima假说，该假说认为Flat minima意味着更好的泛化能力，尚未得到证明。

这篇Blog关注优化问题。

ML/DL中的很多问题最后都可以转化为一个优化问题。如果该问题是凸的，意味着有全局最优解；如果是非凸的，找不到全局最优解。凸优化的解决方式是简单的，但是非凸优化的解决是困难的。

所以，首先讨论一下深度神经网络(后续以DL来代替)的损失函数与非凸的关系？

假设激活函数是线性激活，损失函数是MSE，则该损失函数是凸的，在三维坐标中，会形成一个"碗"形结构，"碗底"就是最优值对应的参数。从非齐次线性方程组解的存在性讨论，形式为Ax=b，激活函数是线性的时候，形成的是线性方程组，b是数据标签，优化的目的是寻找最好的A。通常Ax=b会形成一个超定方程组，解决方法是最小二乘法或者说拟合的观点寻找次优解。那具体的这个方程组是怎么样的呢？

假设网络结构为三层：输入层两个单元，代表二维输入；隐藏层三个单元；输出层一个单元；输入层和隐藏层之间的权值为(w11,w12,w21,w22,w31,w32)，输出和隐藏层之间的权值为(v11,v12,v13)，激活函数是线性的。

假设给定数据为：(x1,x2,y)共有N个，其中N>2，x1和x2表示数据的两个维度，y表示样本标签。

则方程组的形式为:   x1\*(w11\*v11+w21\*v12+w31\*v13)+x2\*(w12\*v11+w22\*v12+w32\*v13)=y

那么，需要求解的参数就是：w11\*v11+w21\*v12+w31\*v13和w12\*v11+w22\*v12+w32\*v13。假设网络为四层或者更多层，方程组的形式仍旧如上，需要求解的参数需要加上更多的权值相乘的项，不过参数个数仍然为2，由输入维度确定。

现在由线性世界进入非线性的世界，假如使用Sigmoid激活函数呢？得到的就是一个非齐次，非线性的超定方程组了(直观上感受应该问题比较复杂)。

StackExchange上的针对该问题的[回答](https://stats.stackexchange.com/questions/106334/cost-function-of-neural-network-is-non-convex)高票中提到：神经网络的损失函数一般来说是既非凸也非凹的。类比于y=sin(x)，有很多的局部最大值和局部最小值，对于y=sin(x)，局部最大值等于全局最大值且局部最小值等于全局最小值，同时意味着所有的局部最小值(最大值)相等。对于非凸的损失函数来说，有很多的局部极小值，如何理解呢？对于一个神经网络，可以通过交换隐藏层的节点得到相同的局部极小值。交换节点之后，对应权值发生变化，在损失函数对应的曲面空间中，形成多个局部最小值。

Quora针对该问题的[回答](https://www.quora.com/Why-is-a-neural-network-and-in-general-a-deep-network-non-convex)提到：神经网络的非凸性来源于非线性激活函数。

既然大多数DL中损失函数都是非凸的，非凸为什么很难优化？

持续好多年，学术界认为DL在优化的时候包含很多局部极小值，使得优化算法容易陷入到这些局部极小值点中，难以自拔。2014年NIPS的文章《Identifying and attacking the saddle point problem in high-dimensional non-convex optimization》中提出，高维非凸优化的困难之处在于存在大量鞍点而非局部极小值。鞍点处满足梯度为零但是Hessian矩阵特征值有正有负，局部极小值处满足梯度为零且Hessian矩阵的特征值全部为正。

梯度优化算法在鞍点附近，梯度很小，Loss变化很小，形成"卡住了"的现象(而且针对高维情形，鞍点附近的平坦区域可能非常大)。这种现象和局部极小值处的现象一致，或许这就是导致学术界很长一段时间认为高维非凸优化困难的原因是存在大量局部极小值的原因。二者的区别在于，虽然"卡住了"，但是还是可以从鞍点出走出来(加扰动)，有可能进入另一个鞍点附近(囧)，但是局部极小值不可以！

与该话题相关的另外一篇文章《The Loss Surfaces of Multilayer Networks》中提到三个贡献：第一，在大网络中，大部分的局部最小值是相等的，因此在测试集上表现相似。第二，相比于小网络，大网络陷入局部最小值的可能性较低。第三，在训练集上没有必要找全局最小值，这可能导致过拟合。

在这篇文章中，作者群对要研究的问题做了三个假设，但是在结论部分给出，假设下的模型讨论和真实模型高度相似，因此具有一定的启发性意义。

如何有效的研究上述问题呢？

在最近的一篇Arxiv文章《VISUALIZE THE LOSS LANDSCAPE OF NEURAL NETS》中提到：网络的架构决定了损失函数是否训练容易；训练参数(batch size, learning rate, optimizer)决定泛化性能。但是这些因素是如何影响Loss Landscape呢？能否以可视化的方式呈现？
文章中给出了一张令人印象深刻的图片如下。

![resnet-56](http://wx3.sinaimg.cn/mw690/aba7d18bgy1fn068o66xtj20xf0i0dsx.jpg)

作者群在文章中指出：随着网络的加深，Loss Landscape突然从接近凸的状态跳转到混乱的状态，而ResNet中的Skip Connections正是阻止了这种跳转，这也解释了为什么使用Skip Connections可以训练很深的网络结构。

《On Large-Batch Training for Deep Learning:Generalization Gap and Sharp Minima》中讨论了Batch Size和神经网络泛化能力的关系，文章用实验证明：Batch Size越大，越有可能收敛到Sharp minimizer；相反，收敛到Flat minimizer。这是由梯度估计的内在的noise引起的。


总结：从低维视角观察到的结论不能直接推广到高维视角，要有选择的推广。比如Bengio的Saddle Point理论。至于Saddle Point是不是真的正确，还需要进一步研究。为什么使用一阶梯度优化方法可以优化一个高度非凸的损失函数且效果不错？网络架构，学习率，Batch Size等参数是如何影响神经网络的训练和泛化性能的？很多问题并没有一个很形式化或者数学上的严格的证明，但是可以看到的是学术界还在以各种方式对这些问题进行探讨。


参考:

1.[有哪些学术界都搞错了，忽然间有人发现问题所在的事情？](https://www.zhihu.com/question/52782960)

2.[Real Valued Test Functions](https://dev.heuristiclab.com/trac.fcgi/wiki/Documentation/Reference/Test%20Functions)

3.[神经网络有什么理论支持？](https://zhuanlan.zhihu.com/p/27609166)

4.[非齐次线性方程组解的存在性](https://baike.baidu.com/item/%E9%9D%9E%E9%BD%90%E6%AC%A1%E7%BA%AA%E7%A8%8B%E7%BB%84)

5.[训练神经网络如何确定Batch Size？](https://zhuanlan.zhihu.com/p/27763696)









