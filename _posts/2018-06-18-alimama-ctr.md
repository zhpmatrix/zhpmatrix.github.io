---
layout: post
title: "阿里妈妈CTR预测"
tags: [数据挖掘]
excerpt: "比赛复盘"
date: 2018-06-18 12:24:00
mathjax: true
---

端午节，阳光正好，坐在窗户旁边，隔窗正是小蠡湖，波光粼粼，荡漾起伏。刚刚研讨班上一块做了比赛复盘，重新梳理了比赛方案，思考后续改进的地方，通过讨论冠亚军方案，得到新的启发。遂写此文，记之。

### Task

#### 赛题内容

给定广告点击相关的用户（user），广告商品（ad），检索词（query），上下文内容（context）和商店（shop）信息的条件下预测广告产生购买行为的概率（pCVR），形式化定义如下：

    pCVR = P(conversion=1|user, ad, query, context, shop)

挑战的难点有两个：第一是日常的转化率预估；第二是特殊日期的转化率预估。挑战的来源在于电商平台中的用户行为偏好，商品长尾分布和热点事件营销等。

#### 评估指标

直接使用二分类问题的log loss作为评估指标。

#### 数据说明

结合淘宝平台的业务场景，举一个例子。
    
    用户光头强进入漫威开的淘宝店，淘宝店一侧是广告栏，广告栏中是一些广告商品，比如浩克的底裤，雷神的锤子等。由于广告商品众多，不同的淘宝店的页面会放置不同的广告商品，比如第一页是浩克的底裤，雷神的锤子放在淘宝店铺的最后一页。

    任务就是预测光头强点击底裤或者锤子广告的概率。

点击概率与什么有关呢？光头强本人的信息有关，比如他是一个漫威迷，但是只喜欢雷神。光头强是男性，喜欢雷神的可能性就比他媳妇儿高很多。白天伐树，晚上看漫威电影，买漫威IP的可能性就很大；与广告商品有关，比如店铺的不同页面，都会有锤子的展示，接受的信息量就要远比底裤多很多；类似的，一些上下文信息，比如广告商品的展示时间，广告商品在店铺的页面编号等；除此之外，当然与店铺本身有关了，漫威的店如果在评价，好评，物流，客服等角度做的比漫威A和漫威B等其他同质店铺要好，可以引来更大的流量，同样的广告商品可以引来更大的点击量。

分析完上述场景，建模方案很显然。这是一个很多特征可以抽取的二分类问题。具体的数据表的字段可以查看[天池平台该比赛的数据说明](https://tianchi.aliyun.com/competition/information.htm?spm=5176.11165320.5678.2.327b6602oVPrkn&raceId=231647)

### Action

最重要的Action应该围绕特征来进行。**浏览完数据表的每个字段后，针对关键字段的问题如下。**

对**广告商品信息表**：

**item\_category\_list**是广告商品的类目列表，类目的组织是树形结构，如何量化这个特征？对于该属性值分析后，值的取值范围固定且数值较小，因此可以直接使用OneHot编码。

**item\_brand\_id**和**item\_city\_id**是广告商品的品牌编号和城市编号，如何量化这个特征？如果One-Hot编码，会产生大量稀疏特征，能否以Embedding的方式构建特征？对该特征的处理采用Label Encoder，使得对应属性值连续。Label Encoder构建了一个属性所有值的字典，每个属性值对应一个字典编号，处理之后的属性值连续。

对**上下文信息表**：

**context\_timestamp**是广告商品的展示时间，含义是什么？如何对这个特征进行充分的挖掘？该特征的实际含义是用户点击广告的具体时间。

**context\_page\_id**是广告商品的展示页面编号，如何挖掘？回归值。

**predict\_category\_property**是查询词预测的类目属性列表，含义是什么？如何有效挖掘这个特征？比赛中这个特征比较复杂，比如值的分布情况差异较大。另外对该特征的理解不到位，因此，比赛中没有处理这个特征。赛后，有人说这个特征的重要性比较高。给定一个上下文，用户输入关键词，进入店家，然后看到广告，点击或者不点击。直观上分析，关键词对于最终的点击会有关系，但是考虑到更为直接的关系可能是用户和广告的交互行为，因此对这个特征没有处理。

**店铺信息表**是实体表，需要对店铺的连续性特征进行量化。

**用户信息表**是用户本身的信息。

**基础数据表**是关系表。

在对上述特征进行充分挖掘之后，就是模型选择和模型融合等常见的比赛套路了，在之前的博客中多次谈到过这样的问题，在此不再赘述。

**脱敏数据对于特征构建的影响？**对特征的理解不到位。比如说，用户年龄。给定一个很大的值，但是给出了值的含义，值越大说明用户年龄越大，因此，可以通过对某个数取余数，获取一个相对可信的年龄值。

**遇到的困难？**数据量大是遇到的最大的困难。

从数据规模来看，初赛的训练数据有48万，测试数据集有6万；复赛的训练数据集有1000万，测试数据集有173万。复赛的原始训练数据有20G，考虑到样本不平衡问题，对负样本降采样，保证正负样本的比例在1:20，最终需要处理的数据在2G左右。

另外一个更为重要的处理方法是数据类型的转换，比如从float64转换到float32等。在比赛复盘阶段，讨论到稀疏矩阵同样可能是一个不错的处理方式。

从数据类型上来看，数据的具体时间范围？比赛的难点是**如何利用常规日期的消费习惯预测购物节？**

比赛给了前七天，预测第八天。由于比赛是脱敏数据，猜测第八天可能是双十一当天的消费数据。

### Results

第一赛季排名355/5204，第二赛季排名231/5204。

### 总结与反思

冠军方案的思路是采用迁移学习，模型使用LightGBM单模型，参考四类特征，包括统计特征（浏览商品数等用户行为特征），时差特征（用户两次购物行为之间的时长），排序特征（用户和商品的交互次数），表征特征（更高层级的特征）。采用前七天的数据训练一个LightGBM模型A，将第八天上午的数据喂给模型A，拿到特征A，将特征A喂给一个新的LightGBM模型B，用模型B去预测第八天下午的数据。

这种思路对于CV的同学，可能比较熟悉。区别在于没有fine-tuning的过程。实际上fine-tuning也是不合理的。因为前七天和第八天的数据分布差异很大，因此对冠军抽取的特征也保持怀疑态度。

亚军的思路是用模型A预测第八天上午的数据，得到的概率作为特征，重新训练一个模型。

复盘时，钱师弟提到一个半监督的想法，在数据量不够的时候，用训练的模型去预测测试集，将结果置信度比较高的测试样本放到训练集中重新训练。结合比赛，数据量已经很多了，不过可以做为一个下次尝试的思路。