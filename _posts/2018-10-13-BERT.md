---
layout: post
title: "关于BERT的讨论"
tags: [NLP]
excerpt: "《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》论文阅读想法"
date: 2018-10-13 15:28:00
mathjax: true
---


### 前言

调侃一下新智元和机器之心针对该工作的报道，题目分别是《NLP历史突破！谷歌BERT模型狂破11项纪录，全面超越人类！》和《最强NLP预训练模型！谷歌BERT横扫11项NLP任务纪录》，你俩有意思啊...

来自Google AI Language的文章，本月五号该工作登顶SQuAD1.1榜首，是关于无监督预训练的一个尝试。本周五(昨天)文章在arXiv上放出来，刚好今天周六可以写一些想法。被文章一作Jacob圈粉，于是从翻译他在reddit上回答开始，这个回答可以帮助读者快速掌握文章关键问题，稍后会讨论一些想法。翻译如下：


### 正文

大家好，我是文章的一作：

基本的想法非常简单。近些年来，学界通过预训练语言模型（深度神经网络等），然后在一些下游NLP任务（问答，自然语言推理，情感分析等）上做微调取得了很好的结果。

经典的语言模型范式是自左到右的，例如：

    "the man went to a store"
    P(the|<s>)*P(man|<s> the)*P(went|<s> the man)*...

然而对于一些下游NLP任务，你通常并不想要一个语言模型，而是一个可能更好的针对每个词的上下文表示。如果每个词只能看到它的左侧，很明显很多信息会丢失掉。所以，有些工作训练了一个自右到左的模型，例如：

    P(store | </s>)*P(a|store </s>)*...

现在，你有每个词的两种上下文表示，分别是自左向右和自右向左。你可以把两种上下文表示拼接起来用于下游任务。

但是直觉上，如果我们训练一个深度双向模型，学习到的上下文表示可能会更好。

不幸的是不能按照普通语言模型一样去训练一个深度双向模型，因为深度双向模型会使得词能够间接地“看到他们自己”，那还预测个毛线？

为了解决上述问题，我们可以使用去噪自编码器中的一个简单技巧。我们随机遮挡住输入句子中的一定比例的词，让模型从上下文中重建这些词。我们称之为“masked LM”，实际上就是完型填空。

任务一：Masked LM

    Input:
    the man [MASK1] to [MASK2] store
    Label:
    [MASK1] = went; [MASK2] = a

特别地，我们把遮挡后的输入句子喂给一个深度Transformer用于编码（encoder），然后利用被遮挡位置对应的隐藏层状态(hidden state)去预测对应的词。这和训练一个语言模型很类似呀!

因为语言模型并不能理解两个句子之间的关系，然后句子关系对于一些NLP任务来说相当重要。因此，为了预训练一个句子关系模型，我们利用了一个非常简单的二分类任务。这个任务将两个句子顺次拼接起来，然后预测后一个句子是否是前一个句子的下一个句子。

任务二：预测下一句

    Input:
    the man went to a store [SEP] he bought a gallon of milk
    Label:
    IsNext

    Input:
    the man went to a store [SEP] penguins are flightless birds
    Lable:
    NotNext

然后在大量语料（维基百科语料+许多电子书的集合）上，我们训练了一个大模型（12 Transformer blocks，768-hidden，110M parameters）。该模型能够用于下游NLP任务，只需要几个epoch的微调。

通过上述工作，我们在几个NLP任务上取得了巨大的提升。即使针对特定领域的任务，我们的模型也不要做任何变化。

但是令我们真正惊讶的是，在训练一个更大的模型后(24 Transformer blocks，1024-hidden，340M parameters)，我们在即使非常小的数据集（标注样本数不大于5000）上也取得了巨大的提升。

翻译完。

### 思考

无监督预训练在NLP中的地位非常高，但是相比于视觉领域中的工作，NLP的相关工作取得的进展似乎并不是很大。在本工作之前，还有一些工作引起的关注度都较大，在文章中作者也提到一些。从效果来看，BERT似乎确实很强。

针对该工作，我比较关注的问题如下（也有可能是某些细节我还没有注意到）。

**提升来自海量的数据吗？**我很好奇为啥文章去做训练步骤数目的影响对比试验。文章中可认为模型超参数固定，假定训练步骤数目和训练数据量成正比，也就是说，不需要一个epoch就可以得到一组试验结果，这样的试验是需要的，但是对比试验中的step是在一个epoch内的，还是所有epoch的step的和，似乎没有明确说明，不过我个人倾向于是在一个epoch内完成的试验。个人认为更好的试验是将数据量作为变量，比较收敛的模型结果。

**提升来自模型改进吗？**这里的模型改进是比较含糊的表达，其实我并不认为文章是模型的改进，倒是可以认为是任务的设计改进。这里要考虑的是模型规模是否是提升的一个关键因素，文章中基于模型本身的超参数做了对比试验。

**提升来自好的预训练任务设计吗？**在对比试验中，由于文章认为任务一是提升的最关键因素，所以作者只是比较了有没有任务一的影响，并没有针对任务二的对比试验设计。

**预训练成本怎样？**大模型需要16个tpu，历时四天；更大的模型需要64个tpu，历时四天。直觉上还是比较大的训练成本，但是没有具体的感觉，不知道4块k80或者一块1080ti或者两块p100的时间成本怎样？（因为我就这点硬件，尴尬）期待中文社区搞一个开源后的模型。

**微调成本怎样？**非常小。

**提升来自神秘调参等不为人知的黑科技吗？**不知道，纯属臆测（尴尬）。

### 结论

基于上述问题，个人认为这篇文章的结果还是非常强的，值得期待。但是试验部分有些地方不太认同，至今我不是很确定提升到底来自哪里？（尴尬）另我看到的文章是不是只是个草稿(_不太认可写作质量_)？

延伸思考下，任务一的随机遮挡策略是否略显粗犷，这里有篇很好的文章（_参考二_）讨论如何更好的去使用遮挡策略，相信会有进一步提升，而且实现成本应该不高。


参考一：《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》

参考二：《Data Nosing As Smoothing In Neural Network Language Models》

参考三：[如何评价BERT模型？](https://www.zhihu.com/question/298203515/answer/509613705)








    














