<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ZHPMATRIX blog</title>
    <description>致力于算法，数据和工程的全链路打通</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 14 Apr 2025 09:20:43 +0800</pubDate>
    <lastBuildDate>Mon, 14 Apr 2025 09:20:43 +0800</lastBuildDate>
    <generator>Jekyll v4.4.1</generator>
    
      <item>
        <title>2024年终总结</title>
        <description>&lt;p&gt;按照惯例，每年一篇年终自我总结。过去几年的总结如下:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhpmatrix.github.io/2023/12/31/summary/&quot;&gt;《2023结束了，说点啥》&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU2MTY2ODEzNA==&amp;amp;mid=2247484533&amp;amp;idx=1&amp;amp;sn=25c6db4bcfd0235925b12e5176ddd40f&amp;amp;chksm=fc740d3ccb03842ad7644062860eeb7858656cfcfb955403f759a39ca7c5053e45069acac6e9&amp;amp;scene=21#wechat_redirect&quot;&gt;《我的2022年终总结》&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU2MTY2ODEzNA==&amp;amp;mid=2247484135&amp;amp;idx=1&amp;amp;sn=476a8f6a876d47561ce3fafe1468ef7a&amp;amp;chksm=fc740baecb0382b87a97bd2aa828448305805321fef983b69d1b6f7a2ad139da8503016aa1f5&amp;amp;scene=21#wechat_redirect&quot;&gt;《2022=2021+1》&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhpmatrix.github.io/2018/12/31/summary-2018/&quot;&gt;《二零一九，GO!》&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhpmatrix.github.io/2018/01/02/summary-2018/&quot;&gt;《聊聊二零一七半年纪事》&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhpmatrix.github.io/2017/07/30/semester-summary/&quot;&gt;《写在回家之前-研一下半学期总结》&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhpmatrix.github.io/2016/12/31/summary-2016/&quot;&gt;《写在2016年最后一天》&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU2MTY2ODEzNA==&amp;amp;mid=2247483688&amp;amp;idx=1&amp;amp;sn=29da4edb484e417729a8f40b2ca4af71&amp;amp;chksm=fc740861cb038177acdbcdfa091c1e4caf13f9025e6733781c4f95e8e7c8e672917bc67dfc6a&amp;amp;scene=21#wechat_redirect&quot;&gt;《写码十年》&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;产品技术&quot;&gt;产品&amp;amp;技术&lt;/h3&gt;

&lt;p&gt;今年&lt;a href=&quot;https://zhpmatrix.github.io/2024/04/16/ending/&quot;&gt;离开&lt;/a&gt;了工作三年多的团队，加入到一个新的团队。工作的主线是基于RAG技术做一款垂直财税领域的AI搜索产品（或者说，财税领域版的metaso），作为该产品唯一的算法同学，收获了代码年产量最高的记录，PC端和移动端产品均已上线，并通过开放API的方式支持了其他业务线产品。&lt;/p&gt;

&lt;p&gt;基于一套技术架构，做了面向集团的OA产品，集团内部拿到了创新大赛的银奖，同时拿到了2024年昇腾AI创新大赛-浙江赛区的铜奖。&lt;/p&gt;

&lt;p&gt;过程中围绕架构设计做了较多的&lt;a href=&quot;https://zhpmatrix.github.io/2024/09/07/ai-tax-search/&quot;&gt;思考&lt;/a&gt;，阅读张刚老师的《软件设计：从专业到卓越》时，对软件工程的基本思想有了进一步的理解。同时，临时决定在最后两个月的时间，备考了系统架构师考试。如果24年的技术成长只有一个关键词，那就是&lt;strong&gt;架构设计&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.souyisou.online/&quot;&gt;搜医搜&lt;/a&gt;是24年做的一个比较系统的个人项目。从想法出现，到代码实现，到买机器，买域名，上线推广等，个人solo的过程很值得回忆。和前同事们组队参加了2024年数字医学技术及应用创新大赛-大模型医疗健康项目赛(DiMTAIC)，初赛晋级(350个团队)，遗憾没有进入决赛。整体上，今年继续保持了在医疗业务上的持续投入，和老爸聊天的时候，多次谈到希望家里能有个职业医生，通过持续在医疗业务上的投入，也算是有一些贡献，意义感和使命感确是一件非常重要的事情。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://aitax.17win.com/#/home?Authorization=Bearer+RKgwsKLnZ3uOoZPjU9nPEPP6H/gjZcVgFlgixoqaWSjeXvjxg9jxTA8yJzNOCd94mm7tSpzOgq0w9Q/3XZuY3MitCACEUV88dDvlWFESem/uZVjdHXvDoxFRxGUJLDByEB1GZSxrWvL9mUxlU5ykT9yDV%2BVNCfQLd6YNf7d%2Bss0=&amp;amp;X-App-Key=cb1e9c178a914a61b4db1bd5735f1036&amp;amp;X-Biz-Code=test&amp;amp;X-User-Id=tester&amp;amp;keyword=%E5%B0%8F%E5%9E%8B%E5%BE%AE%E5%88%A9%E4%BC%81%E4%B8%9A%E6%89%80%E5%BE%97%E7%A8%8E%E4%BC%98%E6%83%A0&quot;&gt;爱搜税&lt;/a&gt;做了部分的投流测试，原计划搜医搜也做一部分引流，但是腾讯广告审批没有通过，加上并非刚需，遂暂时放弃。此外，在年底的时候，给自己写了近十年的个人博客增加了Google的广告，不由得感慨一下，搜索，广告和推荐确实是非常好的业务场景。&lt;/p&gt;

&lt;p&gt;在文章写作上，显著降低了产出量。全年写了8篇文章，其中还有4篇还是关于论文笔记。后续文章写作上的一个主要思路将继续维持&lt;strong&gt;量少但精&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/zhpmatrix&quot;&gt;github&lt;/a&gt;今年共计718个follower，增加31个follower。整体贡献量主要来自离职前后这段时间做了nlp-swift项目（目前暂不开源），已经应用于目前在开发的产品中。对比23年，在github上的表现基本一致。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/github24.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;产品侧重点读了《精益产品开发》等关于精益实践的书，临近年尾，花了好长一段时间嗑完了白鸦总的《SaaS工作手册-2.0》，获益良多。&lt;/p&gt;

&lt;h3 id=&quot;团队管理&quot;&gt;团队&amp;amp;管理&lt;/h3&gt;

&lt;p&gt;刚加入新团队，过去一段时间一直是以IC的身份在Coding，产研团队整体比较扁平，沟通比较顺畅。24年继续德老爷子的书，包括《创新与企业家精神》和《管理的实践》两本书。&lt;/p&gt;

&lt;p&gt;产品研发过程中，会议非常少，坚持每天的晨会(站会)确实是非常有效率的事情，这样可以把时间投入到实际的产出中。&lt;/p&gt;

&lt;p&gt;此外，近两年比较好奇数字游民的生活，一直在找机会到杭州数字游民圣地-良渚那边，结识相关村民。&lt;/p&gt;

&lt;h3 id=&quot;生活个人&quot;&gt;生活&amp;amp;个人&lt;/h3&gt;

&lt;p&gt;整体上今年的主线围绕工作，分为换工作前和换工作后，加上家有考生，生活整体上比较平静。&lt;/p&gt;

&lt;p&gt;考试间隙，收到父母搬进了新房子的消息，也算是一件大喜事。老姐，外甥和老弟利用假期时间来杭州玩了几天，特种兵式旅游，太累了。办理了去香港的签证，一直没找到合适的时间。迎着台风天气去了余华的家乡海盐一趟，我喜欢这种混黄的感觉。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/haiyan.jpg?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;肉又多了。写这篇总结时，24年最后一个要完成的目标，进行中。&lt;/p&gt;

&lt;p&gt;读了一些有意思的书，宁高宁的《三生万物》，刘震云的《一地鸡毛》和《一句顶一万句》，《硅谷钢铁侠》，赫拉利的新书《智人之上》，《我们生活在巨大的差距中》等。买了《耶路撒冷三千年》大部头，好久没打开了。微信读书，全年读书208小时，读过46本书，读完28本(有些读完的没有标记，微信读书的这个细节体验可以提升)，阅读217天，笔记320条，日均阅读35分钟，相比去年提升31%。&lt;/p&gt;

&lt;p&gt;时常换好了一身装备去球场打球，结果找不到人组队，以至于尝试换一个运动项目，比如打羽毛球和跳绳，甚至钓鱼这种活动。&lt;/p&gt;

&lt;h3 id=&quot;反思希望&quot;&gt;反思&amp;amp;希望&lt;/h3&gt;

&lt;p&gt;两年前定下一个目标，能够有机会参与到一个产品的从0到1，以至于业界知名。虽然在24年分别做了两个产品的从0到1，但是业界知名还有很长的路要走。两年后，相信可遇不可求，反而有点喜欢&lt;a href=&quot;https://mp.weixin.qq.com/s/ugCmRMVenBcDPLUZmCsEqA?poc_token=HHx4UWejxwodzfOli3lRdRr2PacVZs3Gc48lOttb&quot;&gt;flomo少楠学长的一些想法&lt;/a&gt;，自己受到斯多葛主义的影响间接来自少楠的某个采访。&lt;/p&gt;

&lt;p&gt;相比之前，今年的内耗明显降低了很多。自身实践比较有效的一个方法是：专注自身的目标。目标越清晰，内耗越低。与之相关的是对于网络上的信息的FOMO感越来越弱，信息带来的价值感持续降低，有些时候会产生一种生理性厌恶，相反在微信读书上的时间投入越来越长，花大时间，做高密度信息摄入。对于即时信息的摄入，基本转换为播客这种单一的形式。退出了积攒了很多年的各种所谓技术交流群。&lt;/p&gt;

&lt;p&gt;逐渐步入中年，大抵孤独是常态，要学会享受孤独。多数情况下，其实很难遇到一个能够深入聊天的人，人与人之间其实很难做到感同身受。当然，孤独未必是一件坏事，通过读书和运动来化解，有足够的时间独处和思考，反而降低了孤独时的各种莫名空虚感。&lt;/p&gt;

&lt;p&gt;2025年即将开始，如果用一个词来定下整体的基调，我选择:&lt;strong&gt;拿结果&lt;/strong&gt;。少做事情多思考(看五年，想三年，扎扎实实做一年)，但是决定要做的事情，一定要做到极致。越来越喜欢自己的微信签名：读万卷书，行万里路，见万种人，做一件事。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://zhpmatrix.github.io/about/&quot;&gt;扫码加笔者好友&lt;/a&gt;，茶已备好，等你来聊~&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 31 Dec 2024 19:40:00 +0800</pubDate>
        <link>http://localhost:4000/2024/12/31/summary/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/12/31/summary/</guid>
        
        <category>生活杂谈</category>
        
        
      </item>
    
      <item>
        <title>《Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data》</title>
        <description>&lt;h4 id=&quot;基本信息&quot;&gt;基本信息&lt;/h4&gt;

&lt;p&gt;主题：	表格问答/大模型微调/RAG&lt;/p&gt;

&lt;p&gt;作者:	东南大学，曼彻斯特大学，华为&lt;/p&gt;

&lt;p&gt;会议：	NAACL2024&lt;/p&gt;

&lt;h4 id=&quot;motivation&quot;&gt;Motivation&lt;/h4&gt;

&lt;p&gt;表格-文本生成技术已经在NLP领域被广泛研究，围绕大模型增强的问答系统，融合文本和半结构化的表格数据是一个趋势，但是不同的表格-文本生成技术是如何影响QA系统的，并没有相对系统的对比分析。通过对不同的表格-文本生成技术的实验对比和理论分析，可以形成一些有价值的技术选型建议。&lt;/p&gt;

&lt;h4 id=&quot;主要贡献&quot;&gt;主要贡献&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;通过实验证明，不同的表格-文本生成技术会显著的影响问答系统的效果。其中，人类评估的RSD（Relative Score Difference）的范围从2.8%到9.0%，GPT-4的评估范围从4.8%到16%&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对于领域微调范式，基于LLM和传统预训练语言模型的方法在各种模型设定下，显著优于其他方法。在RAG范式下，虽然基于LLM的方法依然表现优秀，但是基于markdown的方法也表现出超出预期的有效&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;不同方法产生的领域术语和动词的频率的不同，以及在文本切片的语义表征中的质量不同，是不同方法在两类系统存在效果差异的首要因素&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;主要过程&quot;&gt;主要过程&lt;/h4&gt;

&lt;h5 id=&quot;qa系统的两种实现方法&quot;&gt;QA系统的两种实现方法&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;基于领域模型微调的QA系统&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/12051.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;基于RAG的QA系统&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/12052.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;四种表格-文本生成技术&quot;&gt;四种表格-文本生成技术&lt;/h5&gt;

&lt;p&gt;&lt;strong&gt;Markdown&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;直接将表格表示为markdown格式，不需要模型训练，可以利用脚本在无需人工参与的条件下快速完成转换&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Template&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;使用一系列提前设计好的适配表格特征的模版做表格的文本化。比采用markdown的方式能够实现更好的多样性，但是需要人工提前设计多种模版。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TPLM-based&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;利用传统的预训练语言模型，比如T5和BART等，通过在跨领域的表格-文本生成数据集做微调。这种方法能够提供更高的灵活性和领域适配性，但是也要求更多的计算资源。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LLM-based&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;GPT-*系列模型的效果多数情况下优于表现最好的微调的模型，和TPLM-based方法相比，这种方法能够通过in-context learning实现自定义的裁剪，但是同时也存在领域数据泄漏的风险。&lt;/p&gt;

&lt;h5 id=&quot;实验分析&quot;&gt;实验分析&lt;/h5&gt;

&lt;h6 id=&quot;数据集&quot;&gt;数据集&lt;/h6&gt;

&lt;p&gt;ICT-DATA：170个信息与通信技术领域的英文技术文档，每个文档都是由文本和表格组成，其中表格数据约占总数据的18%。&lt;/p&gt;

&lt;p&gt;ICTQA：从QA平台收集9000个答案较长的关于ICT产品的问答对，其中答案均是由专家根据产品文档书写的。选择500个问题做测试集，对应的答案需要同时从表格和文本中提取，剩下的问答对用于领域模型微调。&lt;/p&gt;

&lt;h6 id=&quot;评估指标&quot;&gt;评估指标&lt;/h6&gt;

&lt;p&gt;自动评估指标：利用GPT-4作为评估器，基于生成的答案和参考答案的相似性打分，分值范围为[0-5]，其中0表示生成的答案如”I don’t know the answer.”，1代表最小的相似度，5代表完全准确的生成答案。&lt;/p&gt;

&lt;p&gt;人工评估指标：评分准则同上，利用三个领域专家进行打分。&lt;/p&gt;

&lt;h6 id=&quot;实验设置&quot;&gt;实验设置&lt;/h6&gt;

&lt;p&gt;用于领域微调的基座模型为Meta’s OPT(1.3B-13B)和Llama-2-base（7B,13B），采用QLora用于持续预训练和指令微调。RAG系统使用的生成模型为Llama2-chat（7B，13B和70B）以及GPT-3.5-turbo。采用文本切片切分语料，每个切片长度不高于3000个char，采用BGE向量模型做表征，利用FAISS存储向量。每个问题检索出最相似的3个切片，基于LangChain实现整体流程。&lt;/p&gt;

&lt;h6 id=&quot;实验结果&quot;&gt;实验结果&lt;/h6&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/12053.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中，加粗表示最佳结果，下划线表示次佳结果。基于领域模型微调的方法中，无论是基于人类的评估结果还是GPT-4的评估结果，LLM-based的方法在绝大多数设定下取得最优结果。而在RAG系统中，两种评估方法中，均存在基于Markdown的表示方法取得最优结果的设定。其中RSD(Relative Score Difference)=(Highest Score - Lowest Score)/5。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;领域微调系统中，为什么TPLM-based的方法和LLM-based的方法分别取得次优和最优结果？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/12054.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上表给出了不同方法生成的语料中，领域术语和动词频率分布情况。可以看出LLM-based的方法能够得到的领域术语和动词最多，其次是TPLM-based的方法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RAG系统中，为什么Markdown的方法和LLM-based的方法分别取得次优和最优结果？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/12055.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;采用t-SNE方法做了切片向量的聚类可视化，对于给定Query会发现LLM-based的方法和Markdown-based的方法得到的切片，在语义空间中距离Query相比其他更加地接近。&lt;/p&gt;

&lt;h4 id=&quot;思考应用&quot;&gt;思考&amp;amp;应用&lt;/h4&gt;

&lt;p&gt;（1）在做表格-文本生成的时候，无论是否利用模型微调，基于大模型的方法多数情况下是最佳实践。但是RAG系统中，也可以考虑采用Markdown的方法&lt;/p&gt;

&lt;p&gt;（2）基于表格的问答应用中，将表格（600*25）格按照行转换为json格式（字段名称采用中文表示），检索效果也不错&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://zhpmatrix.github.io/about/&quot;&gt;扫码加笔者好友&lt;/a&gt;，茶已备好，等你来聊~&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 05 Dec 2024 18:35:00 +0800</pubDate>
        <link>http://localhost:4000/2024/12/05/paper-reading/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/12/05/paper-reading/</guid>
        
        <category>论文笔记</category>
        
        
      </item>
    
      <item>
        <title>读粱宁的《真需求》</title>
        <description>&lt;p&gt;花了周五一个晚上和周六一个下午的时间，读完了粱宁老师的《真需求》，这是一篇读书笔记。第一次知道梁老师是几年前读《产品思维30讲》的时候，对于一个菜鸡🐔而言颇有启发。题图是书中附带的一张画，上联是“赚到钱才是真需求”，下联是“真需求才能赚到钱”。&lt;/p&gt;

&lt;p&gt;粱老师定义的真需求是两个商业闭环的交集，分别是&lt;strong&gt;价值-共识-模式闭环&lt;/strong&gt;和&lt;strong&gt;生态位-因果链-定价权闭环&lt;/strong&gt;，前者是产品思维，用于回应用户需求，后者是产业思维，用于回应环境需求，只不过在这本书中只讲了前者。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;做出有价值的东西和做出有商业价值的东西，是两回事。&lt;/strong&gt;能够在市场中变现，用户愿意付费获得的，才叫商业价值。从笔者狭隘的视角来看，这种矛盾无处不在且时常发生。比如llya和altman的故事，实验室成果和商业产品之间的关系，发明人和企业家等。部分创始人纠结的一个地方在于认为自己做的产品非常有价值，但是为什么卖的不好？是因为有价值但是没有商业价值。&lt;/p&gt;

&lt;p&gt;为此要摆脱自己的主观感受和愿望，站到交易对手那一侧来审视手上的一切。&lt;/p&gt;

&lt;p&gt;书中提到《情绪价值》的作者蔡钰提出的产品价值公式：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;产品价值 = 功能价值+情绪价值+资产价值&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;其中功能价值包含四种不同模型，分别是原材料/劳动力模型，专利IP（知识产权）模型，平台/供应链模型，基础设施模型。经典死法是匪兵甲，也就是自己看自己有模有样，但与主角一比，立刻被结构性碾压。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;情绪价值=生理唤起+认知标记+心理账户&lt;/strong&gt;。巧克力味的屎和屎味的巧克力，对于婴儿来说并无区别，但是对于成年人就不同了，因为成年人有了认知标记。生理唤起和认知标记决定了情绪的产生，心理账户决定了用户是否会付费以及愿意付多少。酒是最古老，最普遍的情绪商品。“貂丁”是情绪价值的经典死法。郭德刚老师在相声中有云：&lt;/p&gt;

&lt;p&gt;“大户人家啊，不光吃的讲究，穿的也讲究。夏天穿一条丁字裤，冬天穿一条丁字棉裤，去东北这种极冷的地方还要穿貂丁。什么是貂丁？就是貂皮做的丁字裤，不是大户人家出身的想必没见过。”&lt;/p&gt;

&lt;p&gt;可持续变现就有资产价值。可持续变现需要两个条件如下：&lt;/p&gt;

&lt;p&gt;(1)有一个可供它持续变现的专门市场/二手市场及配套服务&lt;/p&gt;

&lt;p&gt;(2)它在专门市场/二手市场的价值与价格依赖于某个共识&lt;/p&gt;

&lt;p&gt;《瓦尔登湖》中梭罗的房子因为没有产权证，无法满足条件（1），因此虽然具备功能价值和情绪价值，但是不具备资产价值。由于艺术品市场的共识，梵高的画很贵。&lt;/p&gt;

&lt;p&gt;共识是如何形成的？共识和非共识，以及创新的关系是什么？又是一个非常有意思的问题。&lt;/p&gt;

&lt;p&gt;产品创新是指创造新功能，新情绪和新资产。过去百年商品的演化，具有功能价值的商品一直在整合，情绪价值的商品一直在分化。而今天家庭的价值，很大一部分是这个家庭的成员做共建的情绪价值​和资产价值。&lt;/p&gt;

&lt;p&gt;在讲中国手机30年的时候，书中写到一段话，“所以，经济下滑时，产品卖不出去，如果简单地归因于用户没钱了，那肯定是错的。当年更没钱，但真需要的东西，还是要买。是因为你提供的东西，今天的用户没那么需要了。”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;商业创新者的核心能力，其实就两条​：创造价值的能力和领导共识的能力​。&lt;/strong&gt;所有的创新，起始都是​“非共识”。最容易达成的共识​是“你的长项和特性，正好是对方的期望属性。”&lt;/p&gt;

&lt;p&gt;如何界定产品和服务？产品的本质就是服务，二者都要​为用户解决问题。&lt;/p&gt;

&lt;p&gt;创新不赚钱​。创新是一路花钱，花时间​，交各种学费。赚到钱的人，要么是变现自己已经轻车熟路的能力，要么是准备了充足的学费，一直撑到自己轻车熟路的那一天​。而绝大多数创业失败的人，都是做着自己并不熟悉，不清楚深浅的事，一边干，一边交学费​，走到半路，学费不够了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;​什么是商业模式？模式设计主要包含三大模块，分别是能力系统，变现逻辑和​分配机制。&lt;/strong&gt;权力的核心是分配权，评估一个人​有三个要点：技能点，资源盘和影响力​。你能连接的人，不是你的人脉​；你能帮到的人，才是你的人脉。人只能​对自己有需求的人建立关系。&lt;/p&gt;

&lt;p&gt;规划思维中，人会很清楚什么是对的​。完成计划的方向​，就是对的方向。演化思维的特点则是，没有计划，只有一个大概的方向和大概的原则，根据世界的真实反馈进行调整​和迭代。​&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://zhpmatrix.github.io/about/&quot;&gt;扫码加笔者好友&lt;/a&gt;，茶已备好，等你来聊~&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Nov 2024 18:35:00 +0800</pubDate>
        <link>http://localhost:4000/2024/11/02/real-requirements-reading/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/11/02/real-requirements-reading/</guid>
        
        <category>读书笔记</category>
        
        
      </item>
    
      <item>
        <title>再议规则引擎</title>
        <description>&lt;h3 id=&quot;前言&quot;&gt;前言&lt;/h3&gt;

&lt;p&gt;既上一篇讨论规则引擎的文章&lt;a href=&quot;https://zhpmatrix.github.io/2020/10/21/rule-is-all-you-need/&quot;&gt;兜底哲学:规则引擎方法论&lt;/a&gt;，转眼已经过去了四年。这篇文章是笔者最近围绕规则引擎的实践和思考。&lt;/p&gt;

&lt;h3 id=&quot;业务场景&quot;&gt;业务场景&lt;/h3&gt;

&lt;p&gt;在围绕知识中台构建知识资产时，需要对知识文书打标签，由于需要实现的标签非常多，每个标签的代码实现类似如下:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  if check_words_exist(attachment_content,
                [&quot;未开具发票&quot;,&quot;不列收入&quot;,&quot;少列收入&quot;,&quot;少计销售收入&quot;,&quot;少计销售货物收入&quot;,&quot;未入账收入&quot;,&quot;未入账含税收入&quot;,&quot;少申报收入&quot;,&quot;收入未列&quot;,&quot;未申报确认收入&quot;,&quot;未作视同销售处理&quot;,&quot;账外收入&quot;]) or \
                check_two_words_exist(attachment_content,
                                    &quot;销售货物&quot;,
                                    &quot;未申报&quot;,
                                    20) or \
                check_two_words_exist(attachment_content,
                                    &quot;少计&quot;,
                                    &quot;收入&quot;,
                                    20) or \
                check_two_words_exist(attachment_content,
                                    &quot;少记&quot;,
                                    &quot;收入&quot;,
                                    20) or \
                check_two_words_exist(attachment_content,
                                    &quot;收入&quot;,
                                    &quot;未列&quot;,
                                    20) or \
                check_two_words_exist(attachment_content,
                                    &quot;收入&quot;,
                                    &quot;未入&quot;,
                                    20) or \
                check_three_words_exist(attachment_content,[&quot;未&quot;,&quot;申报&quot;,&quot;收入&quot;],[20,20]) or \
                check_three_words_exist(attachment_content,[&quot;未&quot;,&quot;确认&quot;,&quot;收入&quot;],[20,20]) or \
                check_three_words_exist(attachment_content,[&quot;收入&quot;,&quot;未&quot;,&quot;申报&quot;],[20,20]):
                label_mapping[&quot;隐瞒收入&quot;] = True
            if check_words_exist(attachment_content,
                [&quot;私账&quot;,&quot;个人银行帐号&quot;,&quot;个人银行账户&quot;,&quot;微信&quot;,&quot;信用卡&quot;,&quot;私人账户&quot;,&quot;账外收入&quot;]             
                                 ):
                label_mapping[&quot;公转私/个人卡&quot;]=True
            if check_words_exist(attachment_content,[&quot;个人所得税&quot;]):
                label_mapping[&quot;未缴或少缴个人所得税&quot;] = True
            if (doc_type == &quot;重大税收违法失信主体通知书&quot; and check_two_words_exist(case_fea_paragraph,&quot;虚开&quot;,&quot;抵扣&quot;,50)) or \
                (doc_type != &quot;重大税收违法失信主体通知书&quot; and check_two_words_exist(attachment_content,&quot;虚开&quot;,&quot;抵扣&quot;,50)) or \
                (doc_type != &quot;重大税收违法失信主体通知书&quot; and check_words_exist(attachment_content,[&quot;让他人为自己开具&quot;,&quot;向你公司开具&quot;,&quot;开具给你公司&quot;,&quot;取得他人虚开&quot;])) or \
                (doc_type != &quot;重大税收违法失信主体通知书&quot; and check_three_words_exist(attachment_content,[&quot;取得&quot;,&quot;开具&quot;,&quot;发票&quot;],[20,20])) or \
                (doc_type != &quot;重大税收违法失信主体通知书&quot; and check_three_words_exist(attachment_content,[&quot;接受&quot;,&quot;开具&quot;,&quot;发票&quot;],[20,20])) or \
                (doc_type != &quot;重大税收违法失信主体通知书&quot; and check_three_words_exist(attachment_content,[&quot;接受&quot;,&quot;虚开&quot;,&quot;发票&quot;],[20,20])):
                label_mapping[&quot;虚开发票-申报抵扣&quot;] = True
            if check_two_words_exist(attachment_content,&quot;开具与实际经营业务情况不符&quot;,&quot;发票&quot;,50) or \
               check_two_words_exist(attachment_content,&quot;代开与实际经营业务情况不符&quot;,&quot;发票&quot;,50) or \
               check_words_exist(attachment_content,[&quot;对外虚开&quot;,&quot;对外开具&quot;]) or \
               check_two_words_exist(attachment_content,&quot;你公司向&quot;,&quot;开具&quot;,50) or \
               check_two_words_exist(attachment_content,&quot;开具给&quot;,&quot;发票&quot;,50) or \
               custom_check_type(attachment_content,&quot;你公司&quot;,&quot;开具&quot;,&quot;发票&quot;) or \
               custom_check_type(attachment_content,&quot;你单位&quot;,&quot;开具&quot;,&quot;发票&quot;) or \
               (check_two_words_exist(attachment_content,&quot;虚开&quot;,&quot;发票&quot;,50) and label_mapping[&quot;虚开发票-申报抵扣&quot;] == False):
                label_mapping[&quot;虚开发票-对外虚开&quot;] = True
            if (doc_type == &quot;重大税收违法失信主体通知书&quot; and check_words_exist(case_fea_paragraph,[&quot;骗取出口退税&quot;])) or \
                (doc_type == &quot;重大税收违法失信主体通知书&quot; and check_words_exist(illfact_paragraph,[&quot;骗取出口退税&quot;])) or \
                (doc_type != &quot;重大税收违法失信主体通知书&quot; and check_words_exist(attachment_content,[&quot;出口&quot;])) or \
                (doc_type != &quot;重大税收违法失信主体通知书&quot; and check_two_words_exist(attachment_content,&quot;虚开发票&quot;,&quot;申报出口退税&quot;,50))  or \
                  (doc_type != &quot;重大税收违法失信主体通知书&quot; and check_two_words_exist(attachment_content,&quot;骗取&quot;,&quot;出口退税&quot;,50)): 
                label_mapping[&quot;骗取出口退税&quot;] = True
            if (doc_type == &quot;重大税收违法失信主体通知书&quot; and check_words_exist(case_fea_paragraph,[&quot;少缴&quot;,&quot;未缴&quot;,&quot;逃避缴纳&quot;,&quot;偷税&quot;,&quot;逃避追缴欠税&quot;])) or \
                (doc_type == &quot;重大税收违法失信主体通知书&quot; and check_words_exist(illfact_paragraph,[&quot;少缴&quot;,&quot;未缴&quot;,&quot;逃避缴纳&quot;,&quot;偷税&quot;,&quot;逃避追缴欠税&quot;])) or \
                (doc_type != &quot;重大税收违法失信主体通知书&quot; and check_words_exist(attachment_content,[&quot;少缴&quot;,&quot;未缴&quot;,&quot;逃避缴纳&quot;])):
                label_mapping[&quot;少缴应纳税款&quot;] = True
            if doc_type == &quot;催告书&quot;:label_mapping[&quot;强制执行&quot;] = True

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;每个标签的实现都需要开发者和业务方共同参与，经过多轮的口径调整和实现调整，直到标签的实现在召回率和精确率上达到预期水平。&lt;/p&gt;

&lt;p&gt;因此，笔者就会思考问题如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;这是否是一个可以通过规则引擎实现口径开发和代码开发解耦的场景？&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;能否利用LLM的能力实现打标签？（规则引擎只有一个operator，就是llm）&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;实现过程&quot;&gt;实现过程&lt;/h3&gt;

&lt;p&gt;首先，需要一个笔者能够handle的规则引擎，要求基于python实现，整个开源社区基于python的规则引擎比较少，幸运的是找到了&lt;strong&gt;business_rule&lt;/strong&gt;，一个多年前的基于python的规则引擎，虽然star不多，不过完美满足自己的需求，能够在支持condition和action创建，定义operator，通过all和any实现chain，同时带有一个异常简陋的交互式UI。通过增加llm作为operator，实现了面向业务的快速适配。实现效果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/tag_factory_v0.gif?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上述工作称为tag
_factory_v0，仍然属于传统规则引擎。&lt;a href=&quot;https://knowledge-table-demo.whyhow.ai/&quot;&gt;knowledge_table&lt;/a&gt;是近期的一个工作，通过结合llm和rule做dataframe的处理，在交互设计上非常具有启发性。比如通过定义@作为对dataframe的列的引用，一定程度上可以解决标签开发过程中的标签依赖问题。knowledge_table也采用了Python作为后端开发语言，整体代码的质量非常高，但是直接用于笔者的场景，又显得过重。因此笔者基于gradio通过不同的方式实现了类似knowledge_table的交互效果，实用性显著提升，记为tag_factory_v1，实现效果如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/tag_factory.gif?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;进一步地，回到更加灵活的规则引擎方向上，通过拖拽的方式快速拖出来一个DAG是一个极其重要的模块，称为引擎前端。解析DAG并调度运行+运维?是引擎后端。单独两块工作分别拉出来都有不错的开源的工作，但是合并在一起的工作并不多。&lt;/p&gt;

&lt;p&gt;滴滴开源的LogicFlow可以作为一个引擎前端，Dagu作为引擎前端，可以实现DAG和Yaml的双向映射，Yaml是Python开发者的福音。开源版n8n作为前端，对于operator的支持更加的丰富，包括HTTP请求，函数，外部服务，任务等，效果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/n8n.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中HTTP Request的编辑逻辑如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/n8n_http.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;apache-airflow作为引擎后端，能够将Python代码转化为DAG，同时实现DAG的调度和监控，但是没有搭配一个好的前端。DophinScheduler作为为数不多的同时拥有前端和后端的可以作为理想规则引擎的框架，但是近期看到mlflow把DS的前端拆出来，融合实现ML工作流的编排和调度，毕竟mlflow在机器学习任务的编排上比DS要做的更好（定位不同）。&lt;/p&gt;

&lt;p&gt;因此，如果有tag_factory_v2的话，整体上可行的思路如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;和v1类似，独立开发一套适配业务的框架（operator定义清晰的话，ROI也很高）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;采用DS。强化版可以采用类似mlflow+DS的组合&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;前端和后端分离。比如n8n+airflow的组合&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;后记&quot;&gt;后记&lt;/h3&gt;

&lt;p&gt;在调研和实现过程中发现的一个认知问题是：在过去的一些年，过于关注开源产品了。客观角度上看，闭源产品的产品力和技术力应该更好才对，比如这里的n8n。&lt;/p&gt;

&lt;p&gt;在玄难的&lt;a href=&quot;https://www.jiqizhixin.com/articles/2018-12-12-5&quot;&gt;面向不确定性的软件设计几点思考&lt;/a&gt;中提到，整体的演化方向是从“确定性边界向内归纳抽象找相同”转化为“确定性内核向外生长演化”，而对于规则引擎而言同时包含两个特点。&lt;/p&gt;

&lt;h3 id=&quot;参考资料&quot;&gt;参考资料&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://blog.n8n.io/how-to-scrape-data-from-a-website/&quot;&gt;How to scrape data from a website&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://datathrillz.com/considerations-for-building-a-rules-engine-in-python/&quot;&gt;Considerations for building a rules engine in Python&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://zhpmatrix.github.io/about/&quot;&gt;扫码加笔者好友&lt;/a&gt;，茶已备好，等你来聊~&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 30 Oct 2024 18:35:00 +0800</pubDate>
        <link>http://localhost:4000/2024/10/30/rule-engine-v2/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/10/30/rule-engine-v2/</guid>
        
        <category>Python</category>
        
        
      </item>
    
      <item>
        <title>搜医搜-从0到1的故事</title>
        <description>&lt;p&gt;搜医搜是一款基于默沙东诊疗手册的AI搜索产品。通过把默沙东诊疗手册作为主要的的搜索内容库，融合医疗知识图谱和外部搜索引擎结果，在返回搜索结果的同时，给出基于搜索结果的答案。通过搜索结果可以实现答案溯源，在答案质量不佳时，退化为一款标准搜索引擎产品。&lt;/p&gt;

&lt;p&gt;这篇文章讨论搜医搜从想法到变现的0→1的全流程，是一次“一人公司”的路径可行性探索。专注于讲故事，不讲技术。&lt;/p&gt;

&lt;p&gt;文章通过Notion完成，可以直接发布为网页形式。在此就不把内容搬运到博客上了，可以点击&lt;a href=&quot;https://zhpmatrix.notion.site/0-1-11d68ac27add803ba683cfbd4e5ca25c&quot;&gt;《搜医搜-从0到1的故事》&lt;/a&gt;满足好奇心。&lt;/p&gt;

&lt;p&gt;过程中得到了许多朋友们的各种反馈和建议，谢谢你们～🫰&lt;/p&gt;

&lt;p&gt;接下来是一段招募内容：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
if you.meet(&quot;对持续发布一个完整的东西感兴趣&quot;) and (
   you.meet(&quot;网页前端开发&quot;) or
   you.meet(&quot;小程序开发&quot;) or
   you.meet(&quot;浏览器插件开发&quot;) or
   you.meet(&quot;CPU很多&quot;) or
   you.meet(&quot;GPU很多&quot;)
   ) :
   see_below()
else:
    print(&quot;you&apos;re the best!&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://zhpmatrix.github.io/about/&quot;&gt;扫码加笔者好友&lt;/a&gt;，茶已备好，等你来聊~&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 12 Oct 2024 18:35:00 +0800</pubDate>
        <link>http://localhost:4000/2024/10/12/souyisou/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/10/12/souyisou/</guid>
        
        <category>产品设计</category>
        
        
      </item>
    
      <item>
        <title>《Enhancing Asymmetric Web Search through Question-Answer Generation and Ranking》</title>
        <description>&lt;h4 id=&quot;基本信息&quot;&gt;基本信息&lt;/h4&gt;

&lt;p&gt;主题：通过问答对生成的方式强化非对称网页搜索&lt;/p&gt;

&lt;p&gt;作者：Tencent PCG&lt;/p&gt;

&lt;p&gt;会议：KDD2024&lt;/p&gt;

&lt;h4 id=&quot;motivation&quot;&gt;Motivation&lt;/h4&gt;

&lt;p&gt;网页搜索中，通常建模为一个Query和网页内容的非对称文本匹配问题，而非对称文本匹配由于语义空间的不一致，解决的难度高。因此，可以通过问答对生成的方式将一个非对称文本匹配问题转化为对称文本匹配从而降低问题解决的难度，提升网页搜索的效果。&lt;/p&gt;

&lt;h4 id=&quot;主要贡献&quot;&gt;主要贡献&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;提出了QAGeneration模块，可以利用大模型的能力离线生成高质量的QA对&lt;/li&gt;
  &lt;li&gt;提出了QARanking模块，可以无缝集成全文匹配得分和QQ匹配得分，同时不增加在线时延&lt;/li&gt;
  &lt;li&gt;对比SOTA，提出的QAGR方法，取得了8.7%的离线相关性提升和8.5%的在线参与收益提升&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;主要过程&quot;&gt;主要过程&lt;/h4&gt;

&lt;p&gt;QAGR方法由两个模块组成，分别是离线条件下的QAGeneration模块和在线条件下的QARanking模块。离线模块用于给每个doc生成高质量的QA对，在线模块利用这些QA对用于下游检索任务。具体如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/qagr_0.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;qageneration&quot;&gt;QAGeneration&lt;/h5&gt;

&lt;p&gt;离线QA对的构建主要分为三个步骤，分别如下：&lt;/p&gt;

&lt;p&gt;（1）核心段落抽取&lt;/p&gt;

&lt;p&gt;该阶段的目标主要是两个，分别是消除冗余信息以提升生成问题的质量，同时减少下游模型的输入。通过标注5000个样本，训练一个T5模型用于核心段落抽取。实际中使用这个模型做了百亿文档的核心段落抽取。&lt;/p&gt;

&lt;p&gt;（2）Human in the Loop&lt;/p&gt;

&lt;p&gt;针对抽取的核心段落，利用LLM通过提示词调优的方式抽取QA对，但是存在生成的格式和质量不满足要求的问题。通过instruct-tuning，显著提升了模型在垂域内的生成效果。&lt;/p&gt;

&lt;p&gt;（3）问答验证&lt;/p&gt;

&lt;p&gt;虽然通过第二步显著提升了生成的质量，但是仍然有可能生成无效的问题，比如给定生成的问题，在网页内容中找不到答案，以及生成的问题和答案并没有对齐。为了解决这个问题，文章训练了一个QA有效性判别的模型同时辅助人工规则用于过滤生成的问答对。&lt;/p&gt;

&lt;p&gt;其中模型基于T5，输入包含问题，答案以及文档，人工的标注结果中通过的QA对作为正样本，不通过的做为负样本。&lt;/p&gt;

&lt;p&gt;整体流程如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/qagr_1.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;qaranking&quot;&gt;QARanking&lt;/h5&gt;

&lt;p&gt;Ranking分为两个步骤，分别是预排序和排序。&lt;/p&gt;

&lt;p&gt;（1）预排序。计算Query和Question的terms covered，排序之后返回topN个Question。该阶段利用比较轻量的方法减少了不相关的问题用于流入下游检索阶段。&lt;/p&gt;

&lt;p&gt;（2）排序。&lt;/p&gt;

&lt;p&gt;用于排序的模型设计主要围绕对称孪生BERT展开，主体架构是Q-T-S&amp;amp;Q-Q双塔（共享参数），并融合意图特征，最后通过MLP把三路特征作融合。其中Q-T-S指Query-Title-Summary作为BERT的输入，Q-Q是指Query-Question作为BERT的输入，意图分为“基于关键词”和“基于自然语言”。如果是“基于关键词”，则希望得分更多来自Q-T-S，如果是“基于自然语言”，则希望得分更多来自QQ。整体流程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/qagr_4.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;模型的复杂度从O(LH(S+S’)^2)降低至O(LH(S)^2)，其中L是BERT的层数，H是隐藏层的维度，S是Query+Titlte+Summary的长度，S’是Query+Question的长度。实际中S‘«S且由于是双塔结构，在计算的时候可以并行。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/qagr_3.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了训练这个模型，人工标注了query-document的相关性评级，从0到4共有5个级别。为了保证可解释性和稳定性，采用了pairwise和pointwise两种损失函数，其中前者是margin loss，后者是regression loss，回归损失的目的是希望模型能够学到绝对的ranking信息，使得模型具备一定的物理含义。对于只使用margin loss的模型，输出到得分只有在比较的时候才是有用的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/qagr_2.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;实验结果&quot;&gt;实验结果&lt;/h4&gt;

&lt;p&gt;针对该工作，作者分别做了离线评估和在线A/B，具体结果如下所示：由于实验条件较多，这里暂不列举，具体可以参照文章。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/qagr_5.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/qagr_6.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从离线评估来看，基于学术界和工业界的多个数据集，在多个评估量化指标上，对比其他多个方法，该工作均取得了最好的结果。在线实验也进一步证明了该方法的有效性。&lt;/p&gt;

&lt;h4 id=&quot;思考&quot;&gt;思考&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;和大模型微调中，基于self-instruct的方式将非结构化的文本转化为可以微调的数据具有一致性&lt;/li&gt;
  &lt;li&gt;核心段落抽取也可以利用LLM来尝试完成&lt;/li&gt;
  &lt;li&gt;QA对本质上是对非结构化文档的一个索引。从搜索内容角度看，搜索对象可以直接是QA库，也可以是非结构化文档，同时可以是基于QA对作为索引的非结构化文档。此外，未必采用本文的方法，也可以结合自己的场景，采用类似实验中的基线方法。类似方法包含&lt;a href=&quot;https://www.anthropic.com/news/contextual-retrieval&quot;&gt;anthropicai最近的contexual retrieval&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;QARanking不是很优雅。是不是可以直接路由？统一建模的效果？&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 25 Sep 2024 00:35:00 +0800</pubDate>
        <link>http://localhost:4000/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2024/09/25/enhancing-asymmetric-web-search-through-question-answer-generation-and-ranking/</link>
        <guid isPermaLink="true">http://localhost:4000/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/2024/09/25/enhancing-asymmetric-web-search-through-question-answer-generation-and-ranking/</guid>
        
        <category>搜索</category>
        
        
        <category>论文笔记</category>
        
      </item>
    
      <item>
        <title>一次面向垂域搜索系统的实践与思考</title>
        <description>&lt;p&gt;好久没有写文章了，过去几个月时间在solo一个文档搜索产品，随着2.0版本的全流量上线，终于可以喘一口气做一个回顾和复盘。&lt;/p&gt;

&lt;p&gt;产品一方面要直接满足500万B端用户的专业领域知识搜索需求，另一方面作为基础能力间接满足咨询客服等业务场景。通俗意义上，前者的用户体验，接近于从BingChat开始的各类AI搜索类产品，技术范式接近于RAG(Retrieval Augmented Generation)，而无论是AI搜索还是RAG，在过去的很长一段时间，都有大量的文章在讨论，比如笔者的&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU2MTY2ODEzNA==&amp;amp;mid=2247484869&amp;amp;idx=1&amp;amp;sn=4bf5c85a58fa2e5ee477442752d2a832&amp;amp;chksm=fc740c8ccb03859afd52c4294626eb7b9014533cb4615343f4102708a46128db8fe70afa8407&amp;amp;token=1535827598&amp;amp;lang=zh_CN#rd&quot;&gt;RAG在若干医疗场景的实践和思考&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;RAG的关键在R而非G，因为G侧对于大多数目标落地的团队而言空间非常有限，因此能把R做好的团队，RAG的应用大概率做的也不差。围绕R的误解颇多，比如R是个老问题了，现在做没啥意义了；百度和Bing投入那么多人和时间，要追吗？比如花两天时间就能搞定了吧……(hhh)。anyway，这篇文章结合笔者的实践讨论一些自己的理解。&lt;/p&gt;

&lt;p&gt;R分解之后的核心对象（还有重要但非核心的对象）包括Query，搜索服务和文档库，通过搜索服务建立Query和文档库的联系。其中，Query的特点是开放不确定，长尾性以及弱上下文等，文档库的特点是非结构化，强领域特征以及标签化程度低等，搜索服务要解决的问题包括但不限于一对多，非对称，在线，相关性，时效性，多样性等。搜索交付的价值不仅是一个服务，更是一个数据，工程，算法和策略的结合体，一个易用，可理解，可持续演进的系统。&lt;/p&gt;

&lt;p&gt;为了构建这样的一个系统，首先搭建了一套评测体系，体系设计原则在之前的文章中也反复提及，包括人工和机器相结合，低成本自动化可量化以及分层&amp;amp;多粒度。具体包含评测目标，评测指标，评测方式以及评测依赖四部分内容。整个评测体系也并非从刚开始就全部搭建完成，而是和不同阶段的目标做适配以及持续迭代，以能否支撑搜索服务的演化为唯一目的，实际上截止2.0上线，整个评测体系在代码和机制上才比较接近最初设计的整个体系。&lt;/p&gt;

&lt;p&gt;整个搜索架构在经历四个版本的演进之后，实现了对以BingSearch为代表的第三方搜索服务的替代，最终满足单链路多模块，单模块多方法的特点，能够支持多知识库配置以及搜索实例的配置化。&lt;/p&gt;

&lt;p&gt;BingSearch在POC阶段，有助于实现想法的快速验证。但是随着迭代的深入，BingSearch的引入对于服务演进的阻力逐渐加大，比如过滤只能后置，比如为了实现能力兼容需要做较多的比较痛苦的架构上的trade-off，其中也经历了BingSearch大面积宕机的问题，刚开始还以为是自己的服务挂掉了，直到现在服务稳定性尚存在问题。结合成本考量以及自研进度的跟上，最终实现了对BingSearch的替代。每个外部组件的引入，要做得失考虑，希望metaso能够扛得住！&lt;/p&gt;

&lt;p&gt;从1.0到2.0的持续迭代过程中，比较接近于“开着飞机换轮子”。其中推动了基于ES8的混搜方案，多视角知识加工和应用的标准化，建立围绕RAG为核心的应用开发范式等，离线配套工具的构建以及在线流量复制和A/B等也显著提升了迭代效率和研发效能。&lt;/p&gt;

&lt;p&gt;其中，一站式搜索查因工具作为自底向上推动的工作，承载了较多的功能。主要面向非算法背景的同学，比如产品经理，运营同学，前后端开发等，支持搜索服务的全链路追踪，GSB评估等，同时附加功能包含基础能力分析，Prompt调优，DEMO演示等，轻量级的知识运营模块也包含在内。随着工具的发展，除了服务于搜索产品的演进，也逐步以直接或者间接的方式用于其他产品的迭代和项目验收的支持等，早已超出了搜索服务的支持工具属性。&lt;/p&gt;

&lt;p&gt;随着应用研发的展开，多条线的技术范式基本类似，但是每条线都有自己独特的业务属性，因此需要一个框架抽象，通过模块解耦等，实现进行中应用的能力复用，以及支持新增应用的快速开发。整体上这是一个易用，可理解和可演进的框架，具体要求包括：&lt;/p&gt;

&lt;p&gt;（1） 框架和应用分离，应用和应用隔离&lt;/p&gt;

&lt;p&gt;（2）区分能力和内容&lt;/p&gt;

&lt;p&gt;（3）支持各个环节的个性化实现&lt;/p&gt;

&lt;p&gt;早期采用应用和框架协同演进的思路，能够实现进行中应用的能力复用，实际上这也是不得已为之。通过具体应用的开发为抽象提供具体案例，框架侧多走一步后，在具体应用侧验证框架能力。整体上目前依然处于这个阶段，在三个具体应用的反哺下稳步迭代。同时对于复杂度低的应用开发，框架已经基本能够满足快速开发的要求。&lt;/p&gt;

&lt;p&gt;具体来说，在一个开发界面下，通过整合能力和内容，区分具体业务和通用框架能力，实现具体应用的开发。&lt;/p&gt;

&lt;p&gt;内容侧，应用开发者只需要提供搜索数据（MySQL），mapping和用于表征的服务地址，即可在较短的时间内获得一个index，同时支持搜索数据的T+1更新。对于原始搜索数据的加工链路，针对不同的用户群体，分为不同的策略。如果来自业务侧的数据，需要用户侧完成增量链路的搭建，如果是中台数据，则由中台团队完成，也即该配置化模块的开发方自行完成。整体的流程通过一个统一的配置化界面完成，实现了多视角知识加工和应用的标准化。&lt;/p&gt;

&lt;p&gt;能力侧，框架整合了算法中台已有的基础算法服务能力，沉淀的可复用的用于搜索的各个模块的可通用能力，配套周边能力含日志，异常等。&lt;/p&gt;

&lt;p&gt;以上基本讨论完了主要做的工作。围绕之后的计划，搜索产品侧会推进能力和内容侧的持续优化，支持产品侧的体验提升，同时实现更多的业务接入；一站式搜索查因工具的主要目标是两个，分别是易用性提升以加强对搜索产品服务迭代的支撑力度，沉淀标准组件提升可复用性；框架侧通过和具体应用的交互，实现持续框架演化，同时推动框架在更多应用场景的使用。&lt;/p&gt;

&lt;p&gt;这里不谈搜索主链之外的包含AutoSuggestion，相关搜索，分页，人工运营等，搜索产品在用户交互侧是简单的，一个输入框，返回一个搜索列表。在搜索主链稳定的前提下，似乎做的工作都是单点扭螺丝，ROI有限，不是很清楚现有成熟搜索团队的状态是否是这样？但是搜索主链的从0到1，是一个非常复杂的任务，需要良好的架构设计。过去的几个月其实每天都在谈的，不是算法模型训练，不是评估标准设计，而是架构设计。&lt;/p&gt;

&lt;p&gt;所谓架构，就是有什么和没什么，什么与什么之间的关系是什么？是对一个领域的认知的体现，其中关键的是要看到问题的本质。引用玄难大师的观点，一个牛的人是对领域有一个稳定的认知。所以这势必是一件有挑战的事情。&lt;/p&gt;

&lt;p&gt;搜索是一项融合了数据，工程，算法和策略的系统，并非只是交付一个无状态的独立的算法服务。举一个例子，相同搜索Query，在不同的时刻搜索结果列表可能是不一致的，分析原因是什么？可能原因如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;搜索内容是T+1更新的&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;HNSW导航图使用了近似KNN算法（使用暴力搜索，虽然不会引入随机性，但是大规模场景下不适用）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;混搜时引入了高斯时间衰减（时效性提升特别好的一个性质）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;大模型自身生成结果的随机性&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其他的可能因素含：硬件的差异性等，绝对意义的一摸一样，比较难实现。整体上看，搜索系统是内容和能力的混合体，两者都是要能够可演进，但又不是所有的组件都能够正交设计，这样势必会带来一些复杂性。&lt;/p&gt;

&lt;p&gt;在单链路多模块的cascade模式下，一个优化动作具体在哪个环节实现是最佳选择？在&lt;a href=&quot;https://zhuanlan.zhihu.com/p/398041971&quot;&gt;阿里妈妈朱小强的文章&lt;/a&gt;中提到两种设计哲学，分别是城邦自治模式和一盘棋模式，“召回、粗排、精排等每个模块都按照自己对于全局一致性逻辑的理解，在平衡好集合规模、算力消耗、系统性能等约束的情况下独立迭代”，因此这里一定会存在一个在哪个城邦实现治理的问题。实际上，一盘棋模式在其他的应用中已经有用到，把路径做短，单个模块做宽。面向具体目标，精排也许并非必要？这种模式更加敏捷，不过对于单模块的能力要求更高，因为在特定阶段要完成更多的目标优化任务。这也是笔者自身的一个架构演化方向。&lt;/p&gt;

&lt;p&gt;给很多朋友安利过的张刚老师的&lt;a href=&quot;https://book.douban.com/subject/35966115/&quot;&gt;《软件设计：从专业到卓越》&lt;/a&gt;中谈到契约式设计。所谓契约式设计，就是权利和责任，边界问题也是权责问题。跳出来，不带偏见的思考，哪些是我该干的，哪些不是我该干的，拥有上帝视角。“做多”场景包括通过Query扩展增加用于搜索的Query数（多Query），通过增加知识库扩大搜索内容（多知识库）,通过增加召回通道实现召回结果的多样性（多召回通道）等。&lt;/p&gt;

&lt;p&gt;针对多召回通道，一种设计是，固定召回文档数量，随着召回通道数增加，每个召回通道平摊文档量。优点是排序成本不增加的条件下，增加了多样性；缺点是假设召回文档数量等于召回通道数，那么每个召回通道只能分到一个文档，如果前者大于后者呢？这里存在一个显然的bottleneck。因此另外一种设计是，不要限制召回文档数量，在排序阶段通过算力并行实现对召回文档的排序，以算力换多样性。召回的责任之一是提升召回文档的多样性，为下游实现充分的供给，因此有权通过增加召回通道数实现这一目标，且不能因为会增加下游排序的成本而不去选择。因为增加下游排序成本的问题，可以在下游通过算力消耗解决。&lt;/p&gt;

&lt;p&gt;在早期，如果允许召回文档数量较多的条件下，采用第一种设计也可以但是存在潜在的设计瓶颈。因此在该问题中也可以看到，除了数据，算法，策略和工程，算力也是一个部分条件下需要考虑的因素。&lt;/p&gt;

&lt;p&gt;除此之外，还有一些有价值的问题需要思考。在多Query+多知识库+多召回通道的条件下，搜索的最小颗粒度是什么？怎么保证在元素从1扩展到N的时候，架构调整的最小化？能力和内容一定是强耦合的吗？&lt;/p&gt;

&lt;p&gt;接下来的基本方向包括如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;通过一份相同训练数据，按照不同的训练策略得到三个模型分别用于搜索主链的三个关键环节以期全链路的效果提升&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;搜索内容侧的粒度升级，通过更细粒度的知识管理，提升搜索的精细化以及摘要生成的上下文获取简化&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;融合结构化和非结构化信息的搜索架构3.0&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;online learning，融合在线点击feedback信号到搜索主链中（实时数据工程），以及考虑专业搜索用户profile，实现搜索服务的个性化能力提升。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果顺利地话，产品本月能够和C端用户见面。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://zhpmatrix.github.io/about/&quot;&gt;扫码加笔者好友&lt;/a&gt;，茶已备好，等你来聊~&lt;/strong&gt;&lt;/p&gt;

</description>
        <pubDate>Sat, 07 Sep 2024 18:35:00 +0800</pubDate>
        <link>http://localhost:4000/2024/09/07/ai-tax-search/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/09/07/ai-tax-search/</guid>
        
        <category>搜索</category>
        
        
      </item>
    
      <item>
        <title>《A Surprisingly Simple yet Effective Multi-Query Rewriting Method for Conversational Passage Retrieval》</title>
        <description>&lt;h4 id=&quot;一-基本信息&quot;&gt;一. 基本信息&lt;/h4&gt;
&lt;p&gt;会议信息：SIGIR2024&lt;/p&gt;

&lt;p&gt;关键词：对话式文本检索，Query改写&lt;/p&gt;

&lt;h4 id=&quot;二-动机和贡献&quot;&gt;二. 动机和贡献&lt;/h4&gt;

&lt;p&gt;对话式文本检索中存在的一个挑战是，由于在会话历史中，存在指代，遗漏和话题转换等问题，容易导致检索出不相关的文本。通常的一个解法是根据会话历史，通过Query改写，获取一个非情景化（de-contextualized）的Query用于下游检索。但是这种解法在部分场景下可能捕捉到不正确的意图，从而导致无法检索出正确的答案，根本原因在于Query改写是一个离散生成过程，可能无法准确的捕捉到潜在概率分布和词权重。&lt;/p&gt;

&lt;p&gt;在一些工作中已经表明，对于相同的Query，通过整合多个改写后的Query能够进一步提升检索的效果，而beam search可以在没有任何附加成本的条件下，通过在每个生成步骤中考虑最佳的K个句子，而不是用贪心的模式选择最高概率的句子，天然生成多个改写后的Query且同时建模了词权重，方法简单有效且没有过多的计算成本。&lt;/p&gt;

&lt;p&gt;该工作的主要贡献如下：&lt;/p&gt;

&lt;p&gt;（1）比起当前主流的神经QR方法，能够以没有增加额外计算成本的前提下，产生多个Query&lt;/p&gt;

&lt;p&gt;（2）能够以一种非常有效的方式将产生的Query用于sparse和dense检索中
（3）提出的方法在QReCC数据集中拿到了SOTA的结果&lt;/p&gt;

&lt;h4 id=&quot;三-主要方法&quot;&gt;三. 主要方法&lt;/h4&gt;

&lt;h5 id=&quot;1-query改写&quot;&gt;(1) Query改写&lt;/h5&gt;

&lt;p&gt;通过微调一个生成式语言模型在每个轮次根据beam search的得分生成TopN个改写Query，每个改写的Query有一个改写得分如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/cmqr_1.png?raw=true&quot; alt=&quot;1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中，t1,…,tl是预测的token，q(i,j)的&lt;strong&gt;绝对值符号&lt;/strong&gt;是第i个轮次，第j个改写Query的长度，H是会话历史。考虑到RS是句子序列中词概率的乘积，改写得分会增加长度正则避免改写器产生非常短的改写结果。&lt;/p&gt;

&lt;h5 id=&quot;2-sparse检索&quot;&gt;(2) Sparse检索&lt;/h5&gt;

&lt;p&gt;sparse检索依赖bow文本表示，相关性的计算方式通常如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/cmqr_2.png?raw=true&quot; alt=&quot;2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中w(t,q)表示与query相关的词权重，w(t,d)表示与document相关的词权重。改写的Query用于sparse检索时，主要关注w(t,q)。经典的做法是w(t,q)=c(t,q)，其中c表示频数统计。在该工作中，可以从n个改写的query中构建一个加权bow表示的query。具体分为两个步骤：&lt;/p&gt;

&lt;p&gt;（1）每个改写query中的词权重，可以用beam search的得分来表示&lt;/p&gt;

&lt;p&gt;（2）对于词集合中的每个词，词的权重可以通过对n个改写query中对应的词的权重进行求和取平均
本质上，上述方法通过n个query改写的结果，实现了query扩展和词权重的重估计。&lt;/p&gt;

&lt;h5 id=&quot;3-dense检索&quot;&gt;(3) Dense检索&lt;/h5&gt;

&lt;p&gt;dense检索依赖向量表示，相关性的计算方法通常如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/cmqr_3.png?raw=true&quot; alt=&quot;3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其中h(q)表示query的向量表示，h(d)表示document的向量表示。针对第i轮的n个query，向量表示方式如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/cmqr_4.png?raw=true&quot; alt=&quot;4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最终的query的表示是n个query的加权，比之单个query，一定程度上增加了鲁棒性。&lt;/p&gt;

&lt;h4 id=&quot;四-实验结果&quot;&gt;四. 实验结果&lt;/h4&gt;

&lt;p&gt;采用QReCC数据集，共14k个对话，包含80k个问答对，数据集分割之后，得到测试实例8209个。采用的评估指标为MAP，MRR和Recall@10。采用的微调模型为t5-base，beam width为10，sparse检索的实现基于pyserini， dense embedding模型采用GTR(t5-based)。对比的baseline分别如下：&lt;/p&gt;

&lt;p&gt;（1）manual rewrite。数据集中提供，人工改写的query&lt;/p&gt;

&lt;p&gt;（2）T5QR(Manual)。一个比较强的基于t5的query改写模型&lt;/p&gt;

&lt;p&gt;（3）Con-QRR。一个基于t5的利用强化学习优化检索性能之后的模型&lt;/p&gt;

&lt;p&gt;（4）ConvGQR。利用两个t5模型，第一个进行query改写；第二个针对改写之后的query产生answer&lt;/p&gt;

&lt;p&gt;（5）LLM(adhoc)。基于ChatGPT做query改写&lt;/p&gt;

&lt;p&gt;（6）T5QR(LLM)。整体策略同（4），不同点在于基于10k的样本做了蒸馏。&lt;/p&gt;

&lt;p&gt;具体实验结果如下，星号（*）表示最佳结果，下划线表示第二好的结果。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/cmqr_5.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对比分析得到：&lt;/p&gt;

&lt;p&gt;（1）该方法在sparse和dense两类检索中，各个数据集上都比对比的方法要好（除人工），这意味着该方法对于检索有效果上的提升。&lt;/p&gt;

&lt;p&gt;（2）该方法取得了在QReCC数据集上的SOTA。&lt;/p&gt;

&lt;p&gt;（3）虽然人工改写在TREC-CAsT数据集上取得了最好的结果，但是在QReCC数据集上，自动化的方法还是优于人工改写。&lt;/p&gt;

&lt;h4 id=&quot;五-结论和启发&quot;&gt;五. 结论和启发&lt;/h4&gt;

&lt;p&gt;（1）提出了一种不增加额外代价的query改写方法，可以以一种比较简单的方式集成在sparse和dense的检索pipeline中带来效果的提升&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;如何得到一个query改写模型？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;蒸馏LLM的结果/利用线上搜索日志等&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ES混搜架构中，如何利用这种改写方法？&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;dense检索相对直接，对于sparse检索来说，w(t,q)和w(t,d)是ES的内置操作。或者退化为一种计算关键词权重的逻辑用于召回&lt;/p&gt;

&lt;p&gt;（2）两个需进一步探索的想法&lt;/p&gt;

&lt;p&gt;（2.1）探索多query改写在精排阶段的用法&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于n个改写query，每个query和document计算相关性，按照RS加权得到最终的相关性得分&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;（2.2）改写query个数的自动化选择&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;目标和反馈的确定&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;改写后：&lt;/p&gt;

&lt;p&gt;目标: 计算RS的平均分&amp;gt;=平均分阈值&lt;/p&gt;

&lt;p&gt;反馈：K依次递增，判断是否满足条件。如果满足则退出，否则增加K的值，直到满足条件或者等于最大的K
改写中，根据logit来实现自动化等&lt;/p&gt;

</description>
        <pubDate>Mon, 26 Aug 2024 18:35:00 +0800</pubDate>
        <link>http://localhost:4000/2024/08/26/query-rewrite/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/08/26/query-rewrite/</guid>
        
        <category>搜索</category>
        
        
      </item>
    
      <item>
        <title>《FIRST: Faster Improved Listwise Reranking with Single Token Decoding》</title>
        <description>&lt;h4 id=&quot;基本信息&quot;&gt;基本信息&lt;/h4&gt;

&lt;p&gt;主题：LLM用于重排序&lt;/p&gt;

&lt;p&gt;作者：Heng Ji etc.&lt;/p&gt;

&lt;p&gt;机构：UIUC, IBM etc.&lt;/p&gt;

&lt;p&gt;代码：https://github.com/gangiswag/llm-reranker&lt;/p&gt;

&lt;h4 id=&quot;主要贡献&quot;&gt;主要贡献&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;提出一种新的方法能够单独利用LLM生成的第一个标识符的logit来做排序&lt;/li&gt;
  &lt;li&gt;监督微调模型中增加learning to rank的损失函数，在提升rank效果的同时，降低推理时延&lt;/li&gt;
  &lt;li&gt;比cross-encoder更好的排序效果&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;之前的方法有哪些？&lt;/p&gt;

&lt;p&gt;（1）《Improving Passage Retrieval with Zero-Shot Question Generation》，EMNLP2022&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/image.png?raw=true&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;该方式比较符合直觉。针对每个返回的passage，用LLM生成一个问题question，然后利用LLM计算PPL作为question和Question的相关性表示。&lt;/p&gt;

&lt;p&gt;（2）《Holistic Evaluation of Language Models》，Percy Liang，TMLR&lt;/p&gt;

&lt;p&gt;给定Query和Passage，输出Yes或者No&lt;/p&gt;

&lt;p&gt;（3）《Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents》，EMNLP2022&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/image2.png?raw=true&quot; alt=&quot;image2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/image7.png?raw=true&quot; alt=&quot;image7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;主要思路是通过滑窗的方式，从底部向上进行排序，其中window的大小是要返回的topk个搜索结果。本质上是通过端到端的方式
直接输出最终的排序结果。&lt;/p&gt;

&lt;h4 id=&quot;该工作怎么做的&quot;&gt;该工作怎么做的？&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/image3.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(1）利用LLM生成的第一个标识符的logit来做排序&lt;/p&gt;

&lt;p&gt;（2）加入LTR损失函数的模型微调&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/image4.png?raw=true&quot; alt=&quot;image4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/image5.png?raw=true&quot; alt=&quot;image5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;整体上的损失函数是语言模型和LTR损失的加权。这里，LTR的损失函数的weighting方案，提升了排序序号较前的样本的logit的权重。其中，i和j是真实排序序列中的排序序号，si和sj是第一个token的logit分布中对应的标识符的logit。相关的损失函数在度量学习中有更多一些。&lt;/p&gt;

&lt;h4 id=&quot;排序效果怎样&quot;&gt;排序效果怎样？&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/zhpmatrix/zhpmatrix.github.io/blob/master/images/imag6.png?raw=true&quot; width=&quot;400&quot; align=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;最上一行是通过LM直接生成的效果，从上表可以看出，基于各个任务的平均指标，LM+weighting RankNet的效果最佳，同时去掉LM和去掉weighting均会使得排序效果变差。&lt;/p&gt;

&lt;h4 id=&quot;思考延伸&quot;&gt;思考延伸&lt;/h4&gt;

&lt;p&gt;（1）标识符采用A-Z，限制了window的大小&lt;/p&gt;

</description>
        <pubDate>Wed, 26 Jun 2024 00:35:00 +0800</pubDate>
        <link>http://localhost:4000/2024/06/26/reranker-based-on-llm/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/06/26/reranker-based-on-llm/</guid>
        
        <category>搜索</category>
        
        
      </item>
    
      <item>
        <title>结束</title>
        <description>&lt;p&gt;文章从笔者自己的公众号《KBQA沉思录》转发而来，文章主要围绕以下内容展开：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzU2MTY2ODEzNA==&amp;amp;mid=2247484884&amp;amp;idx=1&amp;amp;sn=ab696993ec6c0c4daca14f8649074854&amp;amp;chksm=fc740c9dcb03858ba41add43d6a36f5dd5b5d0a8825142cf357d5cbd7aff04f81f8decb0b7f1&amp;amp;token=1169779643&amp;amp;lang=zh_CN#rd&quot;&gt;Finally，再见&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 16 Apr 2024 17:14:00 +0800</pubDate>
        <link>http://localhost:4000/2024/04/16/ending/</link>
        <guid isPermaLink="true">http://localhost:4000/2024/04/16/ending/</guid>
        
        <category>生活杂谈</category>
        
        
      </item>
    
  </channel>
</rss>
