<DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>[比赛]PAC-2017比赛工具篇-Spark</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="致力于算法，数据和工程的全链路打通">
    <link rel="canonical" href="http://localhost:4000/2017/08/12/spark/">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="ZHPMATRIX blog posts" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <!-- Personal visit times -->
    <script>
	var _hmt = _hmt || [];
	(function() {
  		var hm = document.createElement("script");
  		hm.src = "//hm.baidu.com/hm.js?39e5930446e371d66d738fef008c3ce2";
  		var s = document.getElementsByTagName("script")[0]; 
  		s.parentNode.insertBefore(hm, s);
	})();
	</script>
 </head>


    <body>
    <header class="site-header">

  <div class="wrap">

    <div style="float:floate; margin-top:10px; margin-right:50px;"></div>
    <a class="site-title" href="/">ZHPMATRIX blog</a>
    <a class="site-title" href="/project.html">项目</a>
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
</a>
      <div class="trigger">
        
          <a class="page-link" href="/about/">关于我</a>
        
          
        
          
        
      </div>
    </nav>
  </div>
  <!-- Personal visit times -->
  <script>
  var _hmt = _hmt || [];
  (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?39e5930446e371d66d738fef008c3ce2";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
  })();
  </script>
  <style>
	body{background-color:#84bf97}
  </style>
 </header>


    <!--<div class="page-content" style="background-color:#F8F8FF;">-->
    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>[比赛]PAC-2017比赛工具篇-Spark</h1>
    <p class="meta">Aug 12, 2017</p>
  </header>

  <article class="post-content">
  <p>全国并行应用挑战赛(PAC-2017)指定的官方开源库是Intel开发的基于Spark的开源深度学习库，旨在发挥CPU集群而非GPU集群的计算能力。在过去两年(难道过去很多年?)，无论是部分业界同仁还是某些无良媒体使得大数据的概念遍地生花🌹，当然，如今的AI也被某些利益圈子提出了不切实际的期望设定，已经被自动化所的宗成庆老师在CCAI2017上骂了一通。不管怎样，从技术层面来说，大数据是一个有价值的概念。在此，多扯一句，从互联网产品发展角度来看，学界和业界似乎都共同在做一件事，那就是<strong>打通信息(数据)流</strong>。从早期的门户网站将信息上网到过去的O2O的发展融合线上线下数据，当然这个融合是垂直领域的融合，前不久去参加华为的一个展，看到了数据在多领域的融合，如今的AI，数据挖掘等，则讲究对深层数据的挖掘，能够发现有意义的信息。<strong>站在个人角度，看好由于多领域数据融合产生新的业务价值。</strong>此外，直播平台的发展，也将逐渐走向健康态，良性发展的道路。</p>

<p>扯淡完毕，回归正题，聊技术。</p>

<p>Hadoop适合离线处理，所谓离线，一个是可以处理更多的数据，另一个是批处理。考虑到和HDFS的结合，持久化到硬盘上也是一个技术点儿，此外，由于离线处理，则在应用场景上适合对时效性要求不高的场景，也就是不要求立即做出反应。</p>

<p>与Hadoop相对应的是Spark的特点，Spark是基于内存计算的，所谓内存计算，是在对中间数据的处理上表现和Hadoop不同，Spark更多的将中间数据落在内存中，而Hadoop则将中间数据落在硬盘上。由于基于内存，自然在I/O上的消耗要远低于Hadoop，适合时效性要求高的场景，比如一些在线系统。此外，这种存储模型更加适合机器学习的迭代优化。当然了，Spark并不具备HDFS的存储能力，要借助HDFS等持久化数据。</p>

<p>关于HDFS的抽象，Master节点称为<strong>NameNode</strong>, Worker节点称为<strong>DataNode</strong>。对于一个集群，至少要有一个Master节点，也就是一个NameNode。从单个机器来看，每个机器上可以有多个DataNode，但是至少在一个集群的某台机器上存在一个NameNode，当然了，这台机器上也可以有DataNode的存在。</p>

<p>RDD是Spark中的核心概念，江湖中称弹性分布式数据集，<strong>是分布式内存的抽象概念</strong>。</p>

<p>RDD的特性主要体现在三个方面(敲黑板注意)，第一是血统关系图。也就是说，Spark维护着RDDs之间的依赖关系和创建关系。比如说，inputRDD通过filter操作创建errorsRDD和warningsRDD, 而这两个RDD可以通过union操作创建badLinesRDD,也就是说，badLinesRDD依赖于上述两个RDD。这样的好处显而易见，从血统关系图中可以恢复丢失数据，也就是说，血统关系图其实维护了一个数据映射关系。</p>

<p>第二是延迟计算。Spark中针对RDD的操作主要有两种，分别是Transformation和Action。Transformation操作的典型API有map(), filter(),reduceByKey()等，而Action操作的代表性API有count(), collect(), save()等，对于二者提供的API，可以发现二者的区别。<strong>Transformation的含义如同其名，强调对数据的转换，而Action则强调对数据结果的获取</strong>。<strong>所谓延迟计算是指对RDD的真正计算发生在对该RDD第一次使用Action操作的时候</strong>。一个典型的例子是<strong>加载数据</strong>，也就是说数据在必要的时候，才会被加载进去。但是要对加载这个操作本身（比如，一行加载数据的代码）进行响应, 是通过Spark内部的metadata记录的，也包括对Transformation操作的响应。类比找老板讨论问题的时候，先去和老板约时间，然后在约定时间直接去找老板，而不是直接去找老板(小心被怼，哈哈)。孰优孰劣，显然！优点是减少数据的传输，也就是省的你白跑多次。</p>

<p>第三是缓存。Spark默认每次在RDD上进行Action操作时，重新计算RDD，也就是要重新跑一遍血统关系图。这个时候很显然的方案是缓存技术，RDD.persist()可以实现对RDD的重复利用，而unpersist()是从缓存中移除。至于为什么要默认每次重新计算？至少这种简单粗暴的方法可以省去很多问题的考虑，比如可能的RDD的一致性问题等。</p>

<p>上述RDD的三个特性也是Spark的精华，每个特性都在某种程度上是为了更好的实现内存计算。关于各种操作的API，具体可以Google，毕竟我也记不住(囧)。</p>

<p>至此，战略层面的东西已经讨论完毕，接下来，休息一下，去看看战术层面的东西，也就是在比赛中我们是如何使用Spark完成既定目标的。</p>

<p>假设对银联产品的<strong>整体评价</strong>这个标签进行三分类(好，中，差),具体数据和比赛要求可参看官方网站。则我们的训练数据如下：</p>

<p>texts = 评论文本,整体评价</p>

<p>第一步生成RDD：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data_rdd = sc.parallelize(texts)
</code></pre></div></div>

<p>默认情况下，SparkContext(sc)会根据集群的情况自动设定slice的数值，也就是数据分成多少份了，当然可以自己设定。这里我们采用默认。</p>

<p>第二步生成关键词：生成关键词的逻辑是这样的，对所有评论文本进行停用词过滤后进行词频统计，然后去掉前10个频率最高的词汇，然后按照词频递减的顺序选择990个词汇作为最终的关键词。为啥？因为词频越高意味着在评论文本中出现次数越多，也就是特征了。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>word_to_ic = analyze_texts(data_rdd,stopwords)
</code></pre></div></div>

<p>其中，stopwords就是停用词表。analyze_texts的实现逻辑如下：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def analyze_texts(data_rdd,stopwords):
      def index(w_c_i):
          ((w, c), i) = w_c_i
          return (w, (i + 1, c))
    return data_rdd.flatMap(lambda text_label: text_to_words(text_label[0],stopwords)) \
          .map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b) \
          .sortBy(lambda w_c: - w_c[1]).zipWithIndex() \
          .map(lambda w_c_i: index(w_c_i)).collect()

def text_to_words(review_text,stopwords):
     words = list(jieba.cut(review_text.replace('\n','')))
     # Filter stop words
     words = filterCmt(words,stopwords)
     return words
</code></pre></div></div>

<p>analyze_texts的返回结果是(关键词，(关键词索引序号，词频))，关键词索引序号是按照词频递减依次增加。</p>

<p>sortBy(lambda w_c:-w_c[1])表示按照词频递减排序，其中w_c的结构是(关键词，词频)，’_‘的作用更像是占位符，这种用法在内部函数index的实现中同样有体现（迷之微笑）。关于index给出一个例子如下：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>w_c_i = ( ('hello',234),2) )
res = index(w_c_i)
res = ('hello',(3,234))
</code></pre></div></div>

<p>zipWithIndex()函数将RDD中的元素和这个元素在RDD中的ID（索引号）组合成键值对。例如：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scala&gt; var rdd2 = sc.makeRDD(Seq("A","B","R","D","F"),2)
scala&gt; rdd2.zipWithIndex().collect

Array[(String, Long)] = Array((A,0), (B,1), (R,2), (D,3), (F,4))
</code></pre></div></div>

<p>其中text_to_words的逻辑是针对每条评论，先去掉所有换行符，然后使用分词工具jieba进行中文分词，然后返回分词后的一个列表。</p>

<p>第三步：将关键词分发到每个节点上。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bword_to_ic = sc.broadcast(word_to_ic)
</code></pre></div></div>

<p>broadcast即广播协议。</p>

<p>第四步：将词向量分发到每个节点上。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bfiltered_w2v = sc.broadcast(filtered_w2v)
</code></pre></div></div>

<p>注意这里的词向量是和关键词对应的词向量，词向量预先是针对所有评论文本训练得来。也就是说，此时集群上同时存在关键词和关键词对应的词向量。</p>

<p>第五步：将训练数据转换成下述格式</p>

<p>评论文本的关键词列表,整体评价</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tokens_rdd = data_rdd.map(lambda text_label:([w for w in text_to_words(text_label[0],stopwords)
    if w in bword_to_ic.value], text_label[1]))
</code></pre></div></div>

<p>第六步：关键词填充。为啥呢？有的评论文本长，有的评论文本短，这样通过填充一些没有信息量的符号，使得所有的关键词列表长度一致，达到处理上的方便，比如说在给模型喂数据的时候。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>padded_tokens_rdd = tokens_rdd.map(lambda tokens_label: (pad(tokens_label[0], "##", sequence_len), 
tokens_label[1]))
</code></pre></div></div>

<p>第七步：将训练数据转换成下述格式</p>

<p>评论文本的关键词向量列表,整体评价</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vector_rdd = padded_tokens_rdd.map(lambda tokens_label:([to_vec(w, bfiltered_w2v.value,embedding_dim)
    for w in tokens_label[0]], tokens_label[1]))
</code></pre></div></div>

<p>第八步：将关键词向量列表转换成评论文本矩阵</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sample_rdd = vector_rdd.map(
     lambda vectors_label: to_sample(vectors_label[0], vectors_label[1],embedding_dim))
</code></pre></div></div>

<p>其中，to_sample的实现逻辑如下：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def to_sample(vectors, label, embedding_dim):
  # flatten nested list
  flatten_features = list(itertools.chain(*vectors))
  features = np.array(flatten_features, dtype='float').reshape(
      [sequence_len, embedding_dim])
  return Sample.from_ndarray(features, np.array(label))
</code></pre></div></div>

<p>语法注释：其中chain()可以把一组<strong>迭代对象</strong>串联起来，形成一个更大的迭代器。int不是可迭代对象。*vectors表示访问一个可迭代对象集合。</p>

<p>第九步：分割数据集</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_rdd, val_rdd = sample_rdd.randomSplit([training_split,1-training_split])
</code></pre></div></div>

<p>至此，基于Spark的数据处理工作结束。欢迎开启Spark之旅！</p>

<hr />

<p>参考:</p>

<p>1.<a href="http://ju.outofmemory.cn/entry/178124">一个实际PySpark项目性能调优</a></p>

<p>2.<a href="http://blog.csdn.net/lantian0802/article/details/36376873">PySpark内部实现</a></p>

<p>讨论了RDD，序列化和反序列化，以及PySpark中的几个经典操作。</p>

<p>3.<a href="http://blog.csdn.net/lwnylslwnyls/article/details/8199454">Python命令行参数处理</a></p>

<p>4.<a href="http://www.cnblogs.com/wrencai/p/4231966.html">Spark默认并行度</a></p>

<p>5.<a href="http://blog.csdn.net/JR_lu/article/details/52932148">sklearn中两种模型持久化的方式</a></p>

<p>6.<a href="http://nbviewer.jupyter.org/github/intel-analytics/BigDL-Tutorials/blob/master/notebooks/spark_basics/structured_streaming.ipynb">Spark Streaming</a></p>

<p>BigDL官方给出的Spark Streaming例子，采用<strong>netcat</strong>作为数据流的产生工具，利用<strong>streaming</strong>接收数据流并进行词频统计。</p>

  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  </div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">ZHPMATRIX blog</h2>

    <div class="footer-col-1 column">
      <ul>
	 <li><a href="https://mp.weixin.qq.com/s?__biz=MzU2MTY2ODEzNA==&mid=2247484598&idx=1&sn=ffbf5407ffd399a591930023639b2560&chksm=fc740dffcb0384e9f8fd98446fb0279fff5d4660fa78aed349b2ae15b2192b037900f9d3943f&token=1310413677&lang=zh_CN#rd">微信公众号《KBQA沉思录》</a></li>
        <li><a href="mailto:zhpmatrix@gmail.com">Gmail邮箱</a></li> 
        <li><a href="https://weibo.com/u/2879902091">微博</a></li> 
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/zhpmatrix">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">zhpmatrix</span>
          </a>
        </li>
       </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">致力于算法，数据和工程的全链路打通</p>
    </div>

  </div>
  
</footer>


    </body>
</html>
