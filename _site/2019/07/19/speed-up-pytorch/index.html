<DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>[PyTorch]PyTorch用于大模型训练</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="致力于算法，数据和工程的全链路打通">
    <link rel="canonical" href="http://localhost:4000/2019/07/19/speed-up-pytorch/">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="ZHPMATRIX blog posts" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <!-- Personal visit times -->
    <script>
	var _hmt = _hmt || [];
	(function() {
  		var hm = document.createElement("script");
  		hm.src = "//hm.baidu.com/hm.js?39e5930446e371d66d738fef008c3ce2";
  		var s = document.getElementsByTagName("script")[0]; 
  		s.parentNode.insertBefore(hm, s);
	})();
	</script>
 </head>


    <body>
    <header class="site-header">

  <div class="wrap">

    <div style="float:floate; margin-top:10px; margin-right:50px;"></div>
    <a class="site-title" href="/">ZHPMATRIX blog</a>
    <a class="site-title" href="/project.html">项目</a>
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
</a>
      <div class="trigger">
        
          <a class="page-link" href="/about/">关于我</a>
        
          
        
          
        
      </div>
    </nav>
  </div>
  <!-- Personal visit times -->
  <script>
  var _hmt = _hmt || [];
  (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?39e5930446e371d66d738fef008c3ce2";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
  })();
  </script>
  <style>
	body{background-color:#84bf97}
  </style>
 </header>


    <!--<div class="page-content" style="background-color:#F8F8FF;">-->
    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>[PyTorch]PyTorch用于大模型训练</h1>
    <p class="meta">Jul 19, 2019</p>
  </header>

  <article class="post-content">
  <p>前言：</p>

<blockquote>
  <p>最近部门大佬在内网博客上放了一篇博客，如何使用Tensorflow训练大模型。因此，想PyTorch下应该也有对应技术，于是有了这篇博客。大模型的训练方法，同样适用于一般场景。需要解决的问题包括计算和存储，容易忽略的时间开销在IO。为了得到一个好用的模型，从数据到模型是一个系统工程，即使相关优化技术都用上，也可能比不用任何优化技术的开发周期长的多，比如模型设计不合理，数据不合适，指标有误，代码Bug，系统环境配置等，但是并不妨碍在极限场景下对于该类技术的需求。</p>
</blockquote>

<p>刚好<a href="https://huggingface.co/pytorch-transformers/examples.html#introduction">pytorch-transformer</a>发布，看到文档中有专门讨论了相同的问题，但是整体看下来，虽然huggingface做了对应实现，但是似乎没有严格测试，不管怎样，有了更多可以有所启发的代码可以读一读。</p>

<h3 id="混合精度加速">混合精度加速</h3>

<p>在之前的博客<a href="https://zhpmatrix.github.io/2019/07/01/model-mix-precision-acceleration/">基于混合精度的模型加速</a>篇中，整理了PyTorch下模型加速的过程和细节。在自己前不久实现的ACL2019的一个工作中，<a href="https://github.com/zhpmatrix/BERTem">Github代码地址</a>，尝试了混合精度加速。</p>

<p>整体流程同之前的博客，这里需要着重说明一下代码修改的细节。</p>

<p>(1)输入数据类型转换(fp32-&gt;fp16)</p>

<p>修改<a href="https://github.com/zhpmatrix/BERTem/blob/master/examples/tacred_run_classifier.py">tacred_run_classifier.py</a>中的输入数据类型：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code> 		all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)
        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)
        # FloatTensor(forward)
        all_entity_mask = torch.tensor([f.entity_mask for f in train_features], dtype=torch.float)
        all_entity_seg_pos = torch.tensor([f.entity_seg_pos for f in train_features], dtype=torch.long)
        all_entity_span1_pos = torch.tensor([f.entity_span1_pos for f in train_features], dtype=torch.float)
        all_entity_span2_pos = torch.tensor([f.entity_span2_pos for f in train_features], dtype=torch.float)
        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)
        if output_mode == "classification":
            all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)
        elif output_mode == "regression":
            all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.float)
</code></pre></div></div>
<p>依据自己的需要，将torch.float转化为torch.float16。对于测试代码做同样的处理。</p>

<p>（2）模型forward函数中的数据类型转换</p>

<p>也就是修改<a href="https://github.com/zhpmatrix/BERTem/blob/master/pytorch_pretrained_bert/modeling.py">modeling.py</a>中的BertForSequenceClassification的forward函数实现。个人认为，之所以需要修改这里，是由于自己在这块的相关实现并不优雅。</p>

<p>（3）给apex源码打补丁</p>

<p>在上述两步完成之后，正常情况下会遇到一个问题，如下：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>AttributeError: 'NoneType' object has no attribute 'contiguous'
</code></pre></div></div>

<p>对应的解决方法，参看<a href="https://github.com/NVIDIA/apex/issues/131">issue</a>。</p>

<p>假设一切顺利的话，就可以正常用起来了。给出我的一些测试结果。在给结果之前，需要说明测试条件。</p>

<p>第一：apex官方推荐了两个测试fp16的docker，分别是nvidia出的，和另外一个docker-hub上的。但是自己尝试了各种办法都pull不到nvidia的docker，于是用了后者。据在tensorflow上做加速的同组同学说，nvidia的docker似乎对混合精度加速做了一些优化。</p>

<p>第二：显卡选择，需要tensor core。测试均是在一张TITAN RTX P2上完成。</p>

<p>在具体任务上，延续之前的setting，将train和dev合并共同作为新的train集，test集不变。在fp32
和fp16的两种setting下，比较相同batch_size下，一个epoch的用时或者每个迭代的用时。</p>

<table>
  <thead>
    <tr>
      <th>比较方面</th>
      <th>fp32</th>
      <th>fp16</th>
      <th>备注</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>训练阶段</td>
      <td>1.04it/s</td>
      <td>4.41it/s</td>
      <td>12.76it/s（独占显卡）</td>
    </tr>
    <tr>
      <td>推断阶段</td>
      <td>4.14it/s</td>
      <td>8.63it/s</td>
      <td> </td>
    </tr>
    <tr>
      <td>测试集指标</td>
      <td>0.65/0.55</td>
      <td>0.64/0.53</td>
      <td>格式：micro/macor</td>
    </tr>
    <tr>
      <td>模型大小</td>
      <td>421M</td>
      <td>212M</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>这里借助于apex实现混合精度加速的方法可行，但是不唯一，感兴趣可以进一步探索其他的方式。</p>

<h3 id="xla加速">XLA加速</h3>

<p><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/overview.md">XLA</a>称为加速线性代数库，其实是一个compiler，本来用于优化tensorflow的底层op，但是同样的技术也可以用于PyTorch。该项目还在试验阶段。G家为了推TPU，称通过XLA连接了PyTorch和TPU。<a href="https://github.com/pytorch/xla">pytorch/xla</a>正是对PyTorch的支持，不过为了测试XLA，同样官方推荐了两个虚拟化相关的镜像。</p>

<h3 id="分布式训练">分布式训练</h3>

<p>在异构环境下，需要考虑单机多卡和多机多卡的问题。单机多卡是比较常见的情景，但是多机多卡比较少见。自己一直想要尝试一下多机分布式的训练，在很早的时候，<a href="https://zhpmatrix.github.io/2017/03/06/matrix-multiplication-mpi-openmp-cuda/">矩阵乘法的分布式实践</a>中实现了一些简单的。限于硬件问题，一直没有尝试，最近有大厂的朋友分别在CV和推荐领域遇到需要分布式训练的场景，比如海量的数据等，感觉还是要尝试一下。可以使用的框架是horovod，是一个分布式训练框架，用于支持TensorFlow,Keras,PyTorch,MXNet，主流框架都支持了。同样，PyTorch官方也有自己的一套实现方法。</p>

<p>要实现分布式训练，一般有两种做法：</p>

<p>第一：按照<a href="https://github.com/pytorch/pytorch#from-source">官方文档</a>去编译PyTorch源码。自己尝试了一下，时间有点久。看了后续的配置项，考虑到可能的集群权限问题，于是放弃了(个人不太喜欢折腾配置之类的事情)。期间要保证首先有一个可用的MPI集群。恰逢<a href="https://huggingface.co/pytorch-transformers/examples.html#introduction">pytorch-transformer</a>发布，因此想尝试一下。</p>

<p>遇到的一个问题如下：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>THCudaCheck FAIL file=/opt/conda/conda-bld/pytorch_1550852152579/work/torch/csrc/cuda/Module.cpp line=34 error=10 : invalid device ordinal
Traceback (most recent call last):
  File "run_glue.py", line 475, in &lt;module&gt;
    main()
  File "run_glue.py", line 376, in main
    torch.cuda.set_device(args.local_rank)
  File "/home/zhanghaipeng/.conda/envs/py36_zhp/lib/python3.6/site-packages/torch/cuda/__init__.py", line 264, in set_device
    torch._C._cuda_setDevice(device)
RuntimeError: cuda runtime error (10) : invalid device ordinal at /opt/conda/conda-bld/pytorch_1550852152579/work/torch/csrc/cuda/Module.cpp:34
</code></pre></div></div>

<p>第二：使用horovod。horovod的使用是通过拉取镜像的方式。具体可以读一读<a href="https://github.com/horovod/horovod/blob/master/docs/docker.rst">文档</a>，相比前者，显得要清爽的多。期间要保证首先实现免密登陆。</p>

<p>在配置Docker的过程中，执行下述命令：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>horovodrun --verbose -np 2 -H hpc1:1,hpc4:1 -p 12345 python pytorch_synthetic_benchmark.py
</code></pre></div></div>

<p>会遇到如下问题：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Filtering local host names.
Checking ssh on all remote hosts.
ssh not successful for host hpc4:
Permission denied, please try again.
Permission denied, please try again.
root@hpc4: Permission denied (publickey,password).
</code></pre></div></div>

<p>怀疑是端口映射的问题（暂未解决）。最终期待的运行方式，<a href="https://blog.csdn.net/weixin_38340975/article/details/87971642#root_ssh_117">类似这样</a>。</p>

<p>第三: 在写这篇博客时候，头条的<a href="https://github.com/bytedance/byteps">BytePS</a>也发布了，和Horovod可以对比一下。</p>

<h3 id="梯度累积">梯度累积</h3>

<p>所谓梯度累积，是用只能塞下batch_size=8的GPU，去实现batch_size=32的效果。思路很简单，4个batch后再去更新梯度。但是要注意的是，框架中的backward是针对一个batch的，因此，累积后要除累积步数(=4)。流程如下：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gradient_accumulation_steps=4
for step in total_steps:
	 loss = get_loss(...)
	 if gradient_accumulation_steps &gt; 1:
	     loss = loss/gradient_accumulation_steps
	 if (step+1)%gradient_accumulation_steps == 0:
	     optimizer.step()
</code></pre></div></div>

<p>具体代码可以看<a href="https://github.com/zhpmatrix/BERTem/blob/master/examples/tacred_run_classifier.py">tacred_run_classifier.py</a>中与train相关的逻辑。</p>

<h3 id="梯度checkpoint">梯度Checkpoint</h3>

<p>在神经网络的forward函数中，每层的激活函数值计算之后需要保存下来，当backward时，根据损失函数值和该层对应的激活函数值计算梯度。也就是说，这种情况下显存占用与
层数成正比。当然可以不存储激活值，在backward时，需要激活函数值的时候重新进行forward就可以了。</p>

<p>分析上述讨论，原始的方式是每个层都存储中间计算得到的激活值。比较直接的方法是都不存，不过计算时间感人，其实也就是通过时间换空间，可以用更大的模型了。那么自然有折中的方式，只存部分层的激活函数值。当backward需要激活函数值的时候，取最近的激活值就行。</p>

<p>按照PyTorch官方的一个说法：通过这种方式，可以实现训练4x-10x大的模型。其实，考虑到显存受限，这个区间个人觉得已经很不错了。</p>

<h3 id="总结">总结</h3>

<p>上述这些技术都是可选的，并不一定要选择，比如参考6中分享了一些优化技术。想再次分享一个观点，一个模型的训练是一个系统工程，可以从各个层面去优化，技术性的或者非技术性的。个人认为，相比技术性的优化，非技术性的优化更重要一些，也就是解决问题的一个顶层思路。另一方面，必要时刻思考一下自己的的资源利用率，有没有最大化生成工具的效能，现在反思一下，实际上我们可能在无意间会造成大量的无效碳排放。</p>

<p>对于多数软件系统，我们总是需要在计算，存储和I/O之间进行trade-off。同样对于大模型的训练，可以从软件层进行优化，比如更节约内存，更快收敛的optimizer，model arch等，可以从硬件系统层进行优化，更好的显卡，更大的带宽等。本着对”分层”和”抽象”的认识，可以更好地做出一些优化决策。</p>

<p>参考：</p>

<p>1.<a href="https://juejin.im/post/5cb04cd15188251af26d25d6">apex的实践</a></p>

<p>给出了一些apex实践中遇到的坑。</p>

<p>2.<a href="https://github.com/suvojit-0x55aa/mixed-precision-pytorch">ResNet50的测试结果</a></p>

<p>从作者给出的结果来看，在精度没有显著丢失的前提下，模型大小减少了一倍，同时速度有提升。相比于这篇博客的结果，虽然速度提升不是很明显，但是应该也是合理的，fp32-&gt;fp16是有代价的。</p>

<p>3.<a href="https://github.com/prigoyal/pytorch_memonger/blob/master/tutorial/Checkpointing_for_PyTorch_models.ipynb">pytorch gradient checkpoint</a></p>

<p>主要使用的模块是torch.utils.checkpoint，不但适用于序列模型，同时适用于很多其他的模型。这篇是应该是最好的tutorial，没有之一吧。</p>

<p>4.<a href="https://github.com/cybertronai/gradient-checkpointing">tensorflow gradient checkpoint</a></p>

<p>有非常棒的讲解原理的动图。</p>

<p>5.《Divide-and-Conquer Checkpointing for Arbitrary Programs with No User Annotation》</p>

<p>gradient checkpoint的原始论文，38页。</p>

<p>6.<a href="https://gist.github.com/HudsonHuang/c5137f628667c05c92ed30a2fdb7ffb3">PyTorch性能指南</a></p>

<p>7.<a href="https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255">The Tips On Training Large Batches in PyTorch</a></p>

<p>8.<a href="https://huggingface.co/pytorch-transformers/examples.html#introduction">Training Large Model: introduction, tools and examples</a></p>

<p>huggingface出品的pytorch-transformer中讨论的关于大模型的训练。</p>

<p>9.<a href="https://towardsdatascience.com/9-tips-for-training-lightning-fast-neural-networks-in-pytorch-8e63a502f565">9 Tips For Training Lightning-Fast Neural Networks In Pytorch</a></p>

<p>和博客中的有重叠，不过有新的内容，很实用。</p>

  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  </div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">ZHPMATRIX blog</h2>

    <div class="footer-col-1 column">
      <ul>
	 <li><a href="https://mp.weixin.qq.com/s?__biz=MzU2MTY2ODEzNA==&mid=2247484598&idx=1&sn=ffbf5407ffd399a591930023639b2560&chksm=fc740dffcb0384e9f8fd98446fb0279fff5d4660fa78aed349b2ae15b2192b037900f9d3943f&token=1310413677&lang=zh_CN#rd">微信公众号《KBQA沉思录》</a></li>
        <li><a href="mailto:zhpmatrix@gmail.com">Gmail邮箱</a></li> 
        <li><a href="https://weibo.com/u/2879902091">微博</a></li> 
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/zhpmatrix">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">zhpmatrix</span>
          </a>
        </li>
       </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">致力于算法，数据和工程的全链路打通</p>
    </div>

  </div>
  
</footer>


    </body>
</html>
