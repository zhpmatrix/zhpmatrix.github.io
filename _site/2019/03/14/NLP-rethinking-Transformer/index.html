<DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>[NLP]Rethink系列-Transformer</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="致力于算法，数据和工程的全链路打通">
    <link rel="canonical" href="http://localhost:4000/2019/03/14/NLP-rethinking-Transformer/">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="ZHPMATRIX blog posts" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <!-- Personal visit times -->
    <script>
	var _hmt = _hmt || [];
	(function() {
  		var hm = document.createElement("script");
  		hm.src = "//hm.baidu.com/hm.js?39e5930446e371d66d738fef008c3ce2";
  		var s = document.getElementsByTagName("script")[0]; 
  		s.parentNode.insertBefore(hm, s);
	})();
	</script>
 </head>


    <body>
    <header class="site-header">

  <div class="wrap">

    <div style="float:floate; margin-top:10px; margin-right:50px;"></div>
    <a class="site-title" href="/">ZHPMATRIX blog</a>
    <a class="site-title" href="/project.html">项目</a>
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
</a>
      <div class="trigger">
        
          <a class="page-link" href="/about/">关于我</a>
        
          
        
          
        
      </div>
    </nav>
  </div>
  <!-- Personal visit times -->
  <script>
  var _hmt = _hmt || [];
  (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?39e5930446e371d66d738fef008c3ce2";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
  })();
  </script>
  <style>
	body{background-color:#84bf97}
  </style>
 </header>


    <!--<div class="page-content" style="background-color:#F8F8FF;">-->
    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>[NLP]Rethink系列-Transformer</h1>
    <p class="meta">Mar 14, 2019</p>
  </header>

  <article class="post-content">
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>一.<a href="https://zhpmatrix.github.io/2019/01/27/NLP-rethinking-base-blocks/">Rethink系列-CNN/RNN/GRU/LSTM/BiLSTM</a></p>

<p>二.<a href="https://zhpmatrix.github.io/2019/01/27/NLP-rethinking-attention/">Rethink系列-Attention</a></p>

<p>三.<a href="https://zhpmatrix.github.io/2019/01/28/NLP-rethingking-seq2seq/">Rethink系列-seq2seq</a></p>

<p>四.<a href="https://zhpmatrix.github.io/2019/01/29/NLP-rethinking-copy-and-coverage/">Rethink系列-copy和coverage机制</a></p>

<p>五.<a href="https://zhpmatrix.github.io/2019/01/31/NLP-rethinking-basic-techniques/">Rethink系列-词法/句法/语义</a></p>

<p>六.<a href="https://zhpmatrix.github.io/2019/03/13/NLP-rethinking-Transformer/">Rethink系列-Transformer</a></p>

<p>前言：</p>

<p>在写代码的时候，觉得想法挺多的。但是到写博客的时候又觉得想写的东西其实不多，但是还是将这篇博客发出来，围绕Transformer的工作很多，后续逐渐更新补充一些新的工作吧。</p>

<h3 id="self-attention是怎么提出来的">Self-Attention是怎么提出来的？</h3>

<p>RNN系的三个问题：序列模式导致难以并行；长期依赖的问题；对序列位置的建模是线性的；实际上CNN也可以实现类似的功能，并且具有以下几个优点：容易并行化；可以挖掘局部依赖；对序列位置的建模是对数的。CNN具有很多优点，但是对依赖性的建模不是很好。Transformer的提出在性能上优于单纯的CNN的模型，除了CNN模型的优点之外，对依赖性的建模更好。</p>

<h3 id="transformer的架构特点是什么">Transformer的架构特点是什么？</h3>

<p>从Transformer的整体架构上来看，Encoder端包括N层相似的子层，每个子层包括Self-Attention层和前馈网络层。Decoder端也包括N层相似的子层，每个子层包括Self-Attention层和Encoder-Decoder Attention层以及前馈网络层。在Encoder端，每个位置的数据有自己独特的数据流路径。这些路径在Self-Attention层有依赖关系，但是在前馈层是独立的，因此这些路径可以并行地执行。这也是Transformer并行性的来源之一。</p>

<h3 id="标签平滑正则化label-smoothing-regularizationlsr">标签平滑正则化(Label Smoothing Regularization，LSR)</h3>

<p>在分类问题中，针对每个训练样本，训练过程中会得到一个该样本是所有类别的预测概率向量。针对该预测概率向量，可以有很多工作来做，比如讨论样本的难易程度等。而LSR认为预测概率向量较大的值，表示模型对于当前样本属于某类的置信度比较高，随着训练过程的进行，该预测值会越来越高，而LSR的目的就是阻止这种趋势的发生。所谓正则化，正是模型对这种行为的惩罚。因为从数值角度来看，这种情况的出现会导致CE损失变得较大，而这并不是我们所希望的。从KL散度的角度也可以给LSR一个合理的解释。具体细节可以读论文<a href="https://arxiv.org/pdf/1512.00567.pdf">《Rethinking the Inception Architecture for Computer Vision》</a>。</p>

<h3 id="positional-encoding">Positional Encoding</h3>

<p>针对一个输入句子，每个位置，每个维度的位置嵌入向量的值是一样的，满足sin和cos函数。之所以选择这两个函数，按照官方论文的意思是可以学习到相对位置的概念。在上文中，开篇就提到对序列位置的建模是一个重要的问题。但是自从Transformer出现之后，工业界很多人都希望有一些更好的对位置编码的方案出现，目前的方案似乎并不是最优的。</p>

<h3 id="dropout">Dropout</h3>

<p>在Transformer的实现过程，Dropout无处不在，为了防止模型过拟合，Dropout的使用可以随心所欲。</p>

<h3 id="权值矩阵共享">权值矩阵共享</h3>

<p>在Transformer中，Encoder和Decoder的Embedding层的权重是共享的，同时在Decoder端的Softmax层前的线性层也和前两个部分共享权重。</p>

<h3 id="mask矩阵">Mask矩阵</h3>

<p>Mask矩阵的两个使用场景分别是：处理不定长输入和在LM中防止未来信息的泄露。一般来说，对PAD的符号也需要做Embedding，但是这些PAD的符号本身来说意义不大，那么参与到训练过程中就可能对模型的性能优化有损伤。那么一种解决问题的思路是用标志位表示出哪些位置是PAD的，哪些不是PAD的，通过在梯度反传过程中过滤掉PAD位的信号就OK。还有一个形式上更加漂亮的Mask矩阵就是对角矩阵，该矩阵可以用于语言模型中的Decoder端，防止由于Attention机制的使用导致未来信息的泄露。</p>

<h3 id="参数初始化初始化为0或者初始化为任意随机数">参数初始化(初始化为0或者初始化为任意随机数)</h3>

<p>初始化为0和均匀初始化以及其他各种初始化方式。在PAD后的序列中，PAD位的初始化通过Mask操作后，对损失函数的影响就会被过滤掉。在PyTorch的已经实现的层中，可以看到大部分的初始化都是按照均匀分布初始化，但是PyTorch同时给与了我们选择初始化方式的接口，可以自己定义各种初始化方式。</p>

<h3 id="新工作">新工作</h3>

<p>除了《Attention is all you need》这篇文章，还有Universal Transformer，后续的The evolved transformer,Transformer-XL, Star-Transformer, BERT，GPT-2等工作。相信在后续会有更多的新的工作出现，这篇博客将持续更新。</p>

<p>参考:</p>

<p>1.<a href="https://medium.com/@giacaglia/transformers-141e32e69591">How Transformer Work-Model Used by Open AI and DeepMind</a></p>

<p>2.<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a></p>

<p>这篇博客基于PyTorch实现了一个Transformer的版本，除此之外，读这篇博客的代码也可以学习PyTorch实现模型的很多技巧，比如实现一个优化函数，比如如何实现并行化的计算损失函数，比如如何将Transformer模型解耦从而方便实现，比如数据的Batch/Iter化等，关于seq2seq模型的一些核心技术也有实现，比如Greedy Decoding的实现，BPE数据编码，Attention的可视化等。总之，质量非常高，令人受益匪浅的博客。该团队开发了opennmt-py，在该框架中也实现了Transformer。</p>

<p>处理上述技术细节之外，对一个模型了解到什么程度可以做框架级的实现，也算给出了一个答案。也就是这篇博客，也对我们观察模型给出了一个角度。</p>

<p>3.<a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p>

<p>这篇博客用可视化的方式，通过非常具体的画图方式讲解了Transformer的细节问题。虽然Transformer的原始论文已经写得比较清晰了，但是读了这篇博客，对Transformer的理解又会加深许多。</p>

<p>4.<a href="https://www.zhihu.com/question/305508138">Mask矩阵在深度学习中有哪些应用场景</a></p>

<p>5.<a href="https://medium.com/sfu-big-data/evolution-of-natural-language-generation-c5d7295d6517">Evolution of Natural Language Generation</a></p>

<p>总结了语言模型的发展历史，前DL时代的语言模型需要从HMM开始讲起，HMM模型只关注当前词的前一个词，没有对上下文建模，自然过度到RNNs，继而现今的Self-Attention模型。</p>


  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  </div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">ZHPMATRIX blog</h2>

    <div class="footer-col-1 column">
      <ul>
	 <li><a href="https://mp.weixin.qq.com/s?__biz=MzU2MTY2ODEzNA==&mid=2247484598&idx=1&sn=ffbf5407ffd399a591930023639b2560&chksm=fc740dffcb0384e9f8fd98446fb0279fff5d4660fa78aed349b2ae15b2192b037900f9d3943f&token=1310413677&lang=zh_CN#rd">微信公众号《KBQA沉思录》</a></li>
        <li><a href="mailto:zhpmatrix@gmail.com">Gmail邮箱</a></li> 
        <li><a href="https://weibo.com/u/2879902091">微博</a></li> 
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/zhpmatrix">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">zhpmatrix</span>
          </a>
        </li>
       </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">致力于算法，数据和工程的全链路打通</p>
    </div>

  </div>
  
</footer>


    </body>
</html>
