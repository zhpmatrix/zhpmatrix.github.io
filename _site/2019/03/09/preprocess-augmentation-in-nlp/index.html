<DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>[NLP]聊一聊，预处理和数据增强技术</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="致力于算法，数据和工程的全链路打通">
    <link rel="canonical" href="http://localhost:4000/2019/03/09/preprocess-augmentation-in-nlp/">
    <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="ZHPMATRIX blog posts" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/main.css">
    <!-- Personal visit times -->
    <script>
	var _hmt = _hmt || [];
	(function() {
  		var hm = document.createElement("script");
  		hm.src = "//hm.baidu.com/hm.js?39e5930446e371d66d738fef008c3ce2";
  		var s = document.getElementsByTagName("script")[0]; 
  		s.parentNode.insertBefore(hm, s);
	})();
	</script>
 </head>


    <body>
    <header class="site-header">

  <div class="wrap">

    <div style="float:floate; margin-top:10px; margin-right:50px;"></div>
    <a class="site-title" href="/">ZHPMATRIX blog</a>
    <a class="site-title" href="/project.html">项目</a>
    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
</a>
      <div class="trigger">
        
          <a class="page-link" href="/about/">关于我</a>
        
          
        
          
        
      </div>
    </nav>
  </div>
  <!-- Personal visit times -->
  <script>
  var _hmt = _hmt || [];
  (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?39e5930446e371d66d738fef008c3ce2";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
  })();
  </script>
  <style>
	body{background-color:#84bf97}
  </style>
 </header>


    <!--<div class="page-content" style="background-color:#F8F8FF;">-->
    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1>[NLP]聊一聊，预处理和数据增强技术</h1>
    <p class="meta">Mar 9, 2019</p>
  </header>

  <article class="post-content">
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>在<a href="https://github.com/zhpmatrix/margin-loss-for-sentence-similarity">基于margin-loss的句子相似度</a>这个项目中，为了验证想法，找不到开放数据集，因此自己从新浪爱问爬取了数据。自己爬的数据和学界开放的数据对比，数据显得非常脏。这里有三个含义：第一：数据不规范，比如用词，写作等；第二：中文文本中蕴含了特殊符号等；第三：数据偏见在真实场景下是确实存在的。这里是从数据质量的角度来看，结合具体的场景，可能会有各种影响数据质量的因素。因此，我们需要数据预处理，目的是提升数据质量；</p>

<p>此外，数据规模也是重要的。我们需要数据增强技术来提升数据规模。这是数据增强技术的表面结果，从深层次的角度来看，数据增强技术可以有效防止模型的过拟合，提升模型的鲁棒性。所谓”见多识广”。</p>

<p>因此，为了提升数据质量和数据规模，我们需要预处理技术和数据增强技术。</p>

<h3 id="一预处理技术">一.预处理技术</h3>

<h4 id="1中文">1.中文</h4>

<h4 id="1分词">(1)分词</h4>

<p>中文分词通常是基于词的中文NLP场景下的第一步，虽然目前的中文分词社区仍然对于一些中文语句的分词结果不理想，但是整体上是可用的。自己常用的分词工具是哈工大的LTP。</p>

<h4 id="2停用词过滤">(2)停用词过滤</h4>

<p>停用词是高频，但是无意义的词。在中英文环境中都存在停用词，但是停用词是否要去掉是一个有争议的问题。从理论上分析，去掉停用词，有助于减少VOC的大小，加快收敛，节省存储和计算时间；减少停用词对句子语义表示的影响，有助于提升模型效果。但是实际上，预估停用词对任务的影响是困难的。因此实际场景下，可以做两个版本分别对比。</p>

<h4 id="3特殊符号清洗">(3)特殊符号清洗</h4>

<p>标点符号和词都是符号，但是还要一类特殊符号，诸如笑脸，数字，运算符号等，在一些场景下，需要注意这类特殊符号的过滤。如果这类符号是低频的，那么通过停用词过滤，可以去掉一部分符号，但是如果不能过滤掉，需要显式地去过滤该类符号。在上文提到的项目中，就会遇到这种表情符号特别丰富的场景。</p>

<h4 id="4任务依赖的预处理">(4)任务依赖的预处理</h4>

<p>在分类任务中，可以通过NER技术将待分类文本中出现的地点，组织和人名替换为&lt;place&gt;，&lt;org&gt;和&lt;person&gt;，减少OOV的可能，优点同上。比如，可以应用在<a href="https://github.com/xueyouluo/fsauor2018">AI Challenger 2018的细粒度情感分类比赛</a>中。</p>

<h4 id="2英文">2.英文</h4>

<h4 id="1分词-1">(1)分词</h4>

<p>英文句子以空格作为分割符，分割单词。但是直接通过空格分隔符会导致下述情况下的分词结果的不正确性。比如：</p>

<p><strong>You’re the apple of my eye!</strong></p>

<p>分词后，<strong>eye!</strong>和<strong>eye</strong>就表示不同的单词了。直观上来看，似有不合理之处。因此，显然，要去掉符号。</p>

<h4 id="2标点符号过滤">(2)标点符号过滤</h4>

<p>在(1)中，去掉标点符号之后，得到，</p>

<p><strong>You re the apple of my eye</strong></p>

<p>这样的话，又会遇到<strong>it’s</strong>和<strong>its</strong>的冲突。</p>

<p>因此，好的英文分词要考虑上述两种情况，NLTK对于</p>

<p><strong>it’s my dog!</strong></p>

<p>的分词结果是：</p>

<p><strong>[[it] [’s] [my] [dog] [!]]</strong></p>

<p>这正是我们需要的。</p>

<h4 id="3大小写统一">(3)大小写统一</h4>

<p>在英文场景下需要考虑，有些特殊场景下不适合进行大小写的统一。比如，BERT的预训练模型就提供了两个版本的，分别是统一后和不做统一的。</p>

<h4 id="4标准化stemminglemmatization">(4)标准化(stemming&amp;lemmatization)</h4>

<p>这个是英文场景所特有的，举例如下：</p>

<p>第一种情况：比如对名词eye，有eye和eyes两种形态；</p>

<p>第二种情况：比如对动词take，有token, taken, take三种形态。</p>

<p>通过stemming(词干分析)解决第一种；通过lemmatization(词元分析)解决第二种；</p>

<p>如果说分词可以认为是中文所特有的（不严格成立，英文也可以分词，但是英文分词的必要性显然不如中文分词），那么标准化就是英文所特有的。</p>

<h4 id="5subword">(5)subword</h4>

<p>为了进一步减少VOC的大小，在机器翻译任务中比较流行的subword技术。比如两个单词，分别是person和possion,则subword可以表示为[[pers],[on],[possi]]，具体由<a href="https://github.com/rsennrich/subword-nmt">语料和BPE算法</a>决定。</p>

<h3 id="二数据增强">二.数据增强</h3>

<h4 id="1简单数据增强">1.简单数据增强</h4>

<h4 id="1同义词替换">(1)同义词替换</h4>

<p>同义词的获取可以基于词向量获取其他技术构建的词的量化表示，通过距离计算相似度得到。</p>

<h4 id="2同义词随机插入">(2)同义词随机插入</h4>

<p>将句子中选定的词的同义词随机插入句子任何位置，插入之后，句子变长。</p>

<h4 id="3随机交换词汇位置">(3)随机交换词汇位置</h4>

<p>交换后，句子长度不变。</p>

<h4 id="4随机删除">(4)随机删除</h4>

<h4 id="2任务依赖的增强技术">2.任务依赖的增强技术</h4>

<h4 id="1back-translation">(1)back-translation</h4>

<p>反向翻译用于机器翻译，用于<a href="https://liweinlp.com/?p=5000">中文文本纠错任务</a>。</p>

<h4 id="3复杂技术">3.复杂技术</h4>

<h4 id="1使用预测语言模型做同义词替换">(1)使用预测语言模型做同义词替换</h4>

<h4 id="2作为平滑技术的数据加噪">(2)作为平滑技术的数据加噪</h4>

<h4 id="3基于生成模型vaeganflow进行数据生成">(3)基于生成模型(VAE&amp;GAN&amp;Flow)进行数据生成</h4>

<p>总结：虽然梳理了一些预处理和数据增强的技术，但是实际任务中的技术选取还是要以下游任务的提升为基准点。2018年有一篇文章做CV领域的自动数据增强技术的，基于强化学习。NLP领域或许也可以做一做呀。从工具使用上，在预处理阶段，综合体验NLTK和哈工大的LTP结合使用最棒。可能与自己不载入一个模型就会感觉不舒服有关吧。本来写这篇博客的目的是想写一个预处理和数据增强的模块的，但是现在感觉意义不是很大。</p>

<p>主要参考：</p>

<p>1.《EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks》</p>

<p>2.<a href="https://zhuanlan.zhihu.com/p/59838270">中文对比英文自然语言处理NLP的区别综述</a></p>

<p>3.<a href="https://arxiv.org/abs/1903.09244">《Low Resource Text Classification with ULMFit and Backtranslation》</a></p>

<p>在情感分类任务上，比较了两种数据增强技术，分别是Random Token Perturbations和Backtranslation，证明了Backtranslation的有效性。</p>

<p>4.<a href="https://arxiv.org/pdf/1906.06045.pdf">《Learning to Ask Unanswerable Questions for Machine Reading Comprehension》</a></p>

<p>SQuAD2.0生成不相关的问题。该问题与文章相关，但是在文章中找不到答案。因为SQuAD2.0添加了一类新的问题，该问题的是“no answer”的。</p>

<p>5.<a href="https://arxiv.org/abs/1703.02573">《Data Noising as Smoothing in Neural Network Language Models》</a></p>

<p>6.<a href="https://zhuanlan.zhihu.com/p/75207641">相对全面的总结</a></p>

<p>7.<a href="https://zhuanlan.zhihu.com/p/76957566?utm_source=qq&amp;utm_medium=social&amp;utm_oi=52727124066304">思考为什么要做预处理，预处理做到什么程度的文章，非常棒。</a></p>

<p>8.<a href="https://towardsdatascience.com/why-you-should-avoid-removing-stopwords-aa7a353d2a52">去除停用词对情感分类的影响</a></p>

<p>如非必要，不要做过多的数据预处理。因为无论是对于任务本身，还是对于后续的生产部署，都是坑。</p>

<p>9.<a href="https://github.com/quincyliang/nlp-data-augmentation">NLP中的数据增强总结，包括多个NLP的具体任务</a></p>

<p>10.《Data Augmentation using Pre-trained Transformer Models》</p>

<p>文章比较了基于自编码(BERT)，预训练语言模型(GPT-2)，和预训练seq2seq(BART/T5)的三种模型用于数据增强的效果。具体的，比如对于一个情感分类任务，三种方式都可以做，哪种好一些？文章的结论是：考虑到标签保持的能力和多样性，seq2seq整体上较好。 在具体数据增强的方法上，文章也有一些阐述。整体上文章解决的问题实用性较强，既可以作为一篇Review，也可以作为一篇技术报告来看。此外，文中的一些方法虽然是放在数据增强的角度来考察的，但是理论上应该也可以推广到其他Task上，例如情感迁移等。</p>

<p>11.<a href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247492349&amp;idx=5&amp;sn=c4c7180ea455310ff1b6539fa7168599&amp;chksm=ebb7da29dcc0533fe6bb348788b66246395110c2c1a606e41edfc7b37a5b66643d6736eed487&amp;scene=0&amp;xtrack=1&amp;key=615298b6af513962a2ccd0616eac026eddcbf12936b0b0ef1ee1609eef767b638d63c184ace6204ea701057c1c49124510e951b47ce95f72e2d6d1355b0475c05a1e0109af3c4bbf8de8a219cb70a8e6&amp;ascene=0&amp;uin=MTg2NTIxNzUxNw%3D%3D&amp;devicetype=iMac+MacBookAir7%2C2+OSX+OSX+10.15.2+build(19C57)&amp;version=11020201&amp;lang=zh_CN&amp;exportkey=AyGPkP9lEsk1LxHnwqlXLfU%3D&amp;pass_ticket=L5m3u%2FRnPtgczNrYDLg8yofoGvrUzX%2B1RWX2pbEdauEOQpSI1cCTiC71mkYWrSX%2B">SCIR-深度学习领域的数据增强</a></p>

<p>12.<a href="https://amitness.com/2020/05/data-augmentation-for-nlp/">A Visual Survey of Data Augmentation in NLP</a></p>

<p>总结的非常全面的NLP中的数据增强方法。</p>

<p>13.<a href="https://stats.stackexchange.com/questions/295383/why-is-data-augmentation-classified-as-a-type-of-regularization">Why is data augmentation classified as a type of regularization?</a></p>

<p>数据增强作为一种正则化的观点。</p>

<p>14.<a href="https://amitness.com/2020/05/data-augmentation-for-nlp/">A Visual Survey of Data Augmentation in NLP</a></p>

  </article>

  <!-- mathjax -->
  
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  </div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">ZHPMATRIX blog</h2>

    <div class="footer-col-1 column">
      <ul>
	 <li><a href="https://mp.weixin.qq.com/s?__biz=MzU2MTY2ODEzNA==&mid=2247484598&idx=1&sn=ffbf5407ffd399a591930023639b2560&chksm=fc740dffcb0384e9f8fd98446fb0279fff5d4660fa78aed349b2ae15b2192b037900f9d3943f&token=1310413677&lang=zh_CN#rd">微信公众号《KBQA沉思录》</a></li>
        <li><a href="mailto:zhpmatrix@gmail.com">Gmail邮箱</a></li> 
        <li><a href="https://weibo.com/u/2879902091">微博</a></li> 
      </ul>
    </div>

    <div class="footer-col-2 column">
      <ul>
        <li>
          <a href="https://github.com/zhpmatrix">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
                 viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
              </svg>
            </span>
            <span class="username">zhpmatrix</span>
          </a>
        </li>
       </ul>
    </div>

    <div class="footer-col-3 column">
      <p class="text">致力于算法，数据和工程的全链路打通</p>
    </div>

  </div>
  
</footer>


    </body>
</html>
